{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "Model Fitting Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyhpgBJW6zCw"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "# Model Fitting: Linear regression, Multiple Linear Regression, Polynomial Regression and Bias-variance tradeoff\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrwOwyqe6zC9"
      },
      "source": [
        "___\n",
        "#Tutorial Objectives\n",
        "\n",
        "This is a Tutorial on fitting models to data. We will start with simple linear regression, using least squares optimization. In our way, we'll learn how to calculate the mean-squared error (MSE), explore how model parameters (slope) influence the MSE and learn how to find the optimal model parameter using least-squares optimization.<br>\n",
        "Then, We will see how can we generalize this to Multiple Linear Regression and also explore how higher order regression models i.e. Polynomial Regression works.<br>\n",
        "Finally, we will wrap up this tutorial with Bias Variance trade off which sort of gives you an intutive comparision between different models we discussed.\n",
        "Happy Learning!\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0PHSsIT6zC-"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjOp-7X8UCmE"
      },
      "source": [
        "Let us begin with importing some libraries viz. numpy and matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "FL9Vf3bd6zC_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EcvGT5X6zDB",
        "cellView": "form"
      },
      "source": [
        "#@title Helper function\n",
        "# This is just a helper function that you will require in the following code. \n",
        "# So, make sure to run this code.\n",
        "def plot_observed_vs_predicted(x, y, y_hat, theta_hat):\n",
        "  \"\"\" Plot observed vs predicted data\n",
        "\n",
        "  Args:\n",
        "      x (ndarray): observed x values\n",
        "  y (ndarray): observed y values\n",
        "  y_hat (ndarray): predicted y values\n",
        "\n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(x, y, label='Observed')  # our data scatter plot\n",
        "  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n",
        "  # plot residuals\n",
        "  ymin = np.minimum(y, y_hat)\n",
        "  ymax = np.maximum(y, y_hat)\n",
        "  ax.vlines(x, ymin, ymax, 'g', alpha=0.5, label='Residuals')\n",
        "  ax.set(\n",
        "      title=fr\"$\\hat{{\\theta}}$ = {theta_hat:0.2f}, MSE = {mse(x, y, theta_hat):.2f}\",\n",
        "      xlabel='x',\n",
        "      ylabel='y'\n",
        "  )\n",
        "  ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSWG07Z66zDC"
      },
      "source": [
        "---\n",
        "# Section 1: Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZgxRsn0SaZ7p"
      },
      "source": [
        "#@title Video 1: Linear Regression & Mean Squared Error\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"HumajfjJ37E\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N63V_BB66zDD"
      },
      "source": [
        "**Linear regression** is an old but gold  optimization procedure that we are going to use for data fitting. Least squares (LS) optimization problems are those in which the objective function is a quadratic function of the\n",
        "parameter(s) being optimized.\n",
        "\n",
        "Suppose you have a set of measurements, $y_{n}$ (the \"dependent\" variable) obtained for different input values, $x_{n}$ (the \"independent\" or \"explanatory\" variable). Suppose we believe the measurements are proportional to the input values, but are corrupted by some (random) measurement errors, $\\epsilon_{n}$, that is:\n",
        "\n",
        "$$y_{n}= \\theta x_{n}+\\epsilon_{n}$$\n",
        "\n",
        "for some unknown slope parameter $\\theta.$ The least squares regression problem uses **mean squared error (MSE)** as its objective function, it aims to find the value of the parameter $\\theta$ by minimizing the average of squared errors:\n",
        "\n",
        "\\begin{align}\n",
        "\\min _{\\theta} \\frac{1}{N}\\sum_{n=1}^{N}\\left(y_{n}-\\theta x_{n}\\right)^{2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea7sKsoi6zDE"
      },
      "source": [
        "We will now explore how MSE is used in fitting a linear regression model to data. For illustrative purposes, we will create a simple synthetic dataset where we know the true underlying model. This will allow us to see how our estimation efforts compare in uncovering the real model (though in practice we rarely have this luxury).\n",
        "\n",
        "First we will generate some noisy samples $x$ from [0, 10) along the line $y = 1.5x$ as our dataset we wish to fit a model to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6vOiRzs6zDE",
        "cellView": "code"
      },
      "source": [
        "\n",
        "#this is a random number generator and setting a seed ensures that we get the same pseudonumber sequence.\n",
        "np.random.seed(121)\n",
        "\n",
        "# Let's set some parameters\n",
        "theta = 1.5\n",
        "n_samples = 50\n",
        "\n",
        "# Draw x and then calculate y\n",
        "x = 10 * np.random.rand(n_samples)  # sample from a uniform distribution over [0,10)\n",
        "noise = np.random.randn(n_samples)  # sample from a standard normal distribution\n",
        "y = theta * x + noise\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y)  # produces a scatter plot\n",
        "ax.set(xlabel='x', ylabel='y');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ml9ygCj6zDF"
      },
      "source": [
        "Now that we have our dataset is much noisy,so  we can start trying to estimate the underlying model that produced it. We use MSE to evaluate how successful a particular slope estimate $\\hat{\\theta}$ is for explaining the data, with the closer to 0 the MSE is, the better our estimate fits the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMc1xIZE6zDG"
      },
      "source": [
        "## Exercise 1: Compute MSE\n",
        "\n",
        "In this exercise you have to implement a method to compute the mean squared error i.e. MSE for a set of inputs $x$, measurements (outputs) $y$, and slope estimate $\\hat{\\theta}$. We will then compute and print the mean squared error for 4 different choices of theta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "797eWFCN6zDG"
      },
      "source": [
        "def mse(x, y, theta_hat):\n",
        "  \"\"\"Compute the mean squared error\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): An array of shape (samples,) that contains the input values.\n",
        "    y (ndarray): An array of shape (samples,) that contains the corresponding\n",
        "      measurement values to the inputs.\n",
        "    theta_hat (float): An estimate of the slope parameter\n",
        "\n",
        "  Returns:\n",
        "    float: The mean squared error of the data with the estimated parameter.\n",
        "  \"\"\"\n",
        "  ####################################################\n",
        "  ## TODO for students: compute the mean squared error\n",
        "  # Fill out function and remove the next line by commenting it out\n",
        "  raise NotImplementedError(\"Student exercise: compute the mean squared error\")\n",
        "  ####################################################\n",
        "\n",
        "  # Compute the estimated y\n",
        "  y_hat = ...\n",
        "\n",
        "  # Compute mean squared error\n",
        "  mse = ...\n",
        "  \n",
        "  return mse\n",
        "\n",
        "\n",
        "# Uncomment below  3 lines of code to test your function\n",
        "#theta_hats = [0.8, 1.0, 1.6, 1.2]\n",
        "#for theta_hat in theta_hats:\n",
        "  #print(f\"theta_hat of {theta_hat} has an MSE of {mse(x, y, theta_hat):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9QNjpy16zDK"
      },
      "source": [
        "The result should be:\n",
        "\n",
        "theta_hat of 0.8 has an MSE of 17.08\\\n",
        "theta_hat of 1.0 has an MSE of 9.27\\\n",
        "theta_hat of 1.6 has an MSE of 1.14\\\n",
        "theta_hat of 1.2 has an MSE of 4.02\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If0FE8Z46zDK"
      },
      "source": [
        "We see that $\\hat{\\theta} = 1.6$ is our best estimate from the three we tried. Looking just at the raw numbers, however, isn't always satisfying, so let's visualize what our estimated model looks like over the data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqjlrHzN6zDL",
        "cellView": "code"
      },
      "source": [
        "\n",
        "# Execute this cell to visualize estimated models\n",
        "\n",
        "fig, axes = plt.subplots(ncols=4, figsize=(18, 4))\n",
        "for theta_hat, ax in zip(theta_hats, axes):\n",
        "\n",
        "  # True data\n",
        "  ax.scatter(x, y, label='Observed')  # our data scatter plot\n",
        "\n",
        "  # Compute and plot predictions\n",
        "  y_hat = theta_hat * x\n",
        "  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n",
        "\n",
        "  ax.set(\n",
        "      title= fr'$\\hat{{\\theta}}$= {theta_hat}, MSE = {mse(x, y, theta_hat):.2f}',\n",
        "      xlabel='x',\n",
        "      ylabel='y'\n",
        "  );\n",
        "\n",
        "axes[0].legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-nwKuGI3T3E"
      },
      "source": [
        "As you can see from the above graphs that we have the best fit for $\\hat{\\theta} = 1.6$ which is much close to the actual value i.e. 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj9ji6s-6zDM"
      },
      "source": [
        "While visually exploring several estimates can be instructive, it's not the most efficient for finding the best estimate to fit our data. Another technique we can use is choose a reasonable range of parameter values and compute the MSE at several values in that interval. This allows us to plot the error against the parameter value (this is also called an **error landscape**, especially when we deal with more than one parameter). We can select the final $\\hat{\\theta}$ ($\\hat{\\theta}_{MSE}$) as the one which results in the lowest error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GACGgKCK6zDN"
      },
      "source": [
        "# Execute this cell to loop over theta_hats, compute MSE, and plot results\n",
        "\n",
        "# Loop over different thetas, compute MSE for each\n",
        "theta_hat_grid = np.linspace(-2.0, 4.0)\n",
        "errors = np.zeros(len(theta_hat_grid))\n",
        "for i, theta_hat in enumerate(theta_hat_grid):\n",
        "  errors[i] = mse(x, y, theta_hat)\n",
        "\n",
        "# Find theta that results in lowest error\n",
        "best_error = np.min(errors)\n",
        "theta_hat = theta_hat_grid[np.argmin(errors)]\n",
        "\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(theta_hat_grid, errors, '-o', label='MSE', c='C1')\n",
        "ax.axvline(theta, color='g', ls='--', label=r\"$\\theta_{True}$\")\n",
        "ax.axvline(theta_hat, color='r', ls='-', label=r\"$\\hat{{\\theta}}_{MSE}$\")\n",
        "ax.set(\n",
        "  title=fr\"Best fit: $\\hat{{\\theta}}$ = {theta_hat:.2f}, MSE = {best_error:.2f}\",\n",
        "  xlabel=r\"$\\hat{{\\theta}}$\",\n",
        "  ylabel='MSE')\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sArNWeP46zDN"
      },
      "source": [
        "Now here we can see that our best fit is $\\hat{\\theta}=1.55$ with an MSE of 0.95. This is really closer to the original true value $\\theta=1.5$!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M35E18XB6zDO"
      },
      "source": [
        "---\n",
        "# Section 2: Least-squares optimization\n",
        "\n",
        "\n",
        "While the approach detailed above (computing MSE at various values of $\\hat\\theta$) quickly got us to a good estimate, it still relied on evaluating the MSE value across a grid of hand-specified values. If we didn't pick a good range to begin with, or with enough granularity, we might miss the best possible estimator. Let's go one step further, and instead of finding the minimum MSE from a set of candidate estimates, let's solve for it analytically.\n",
        "\n",
        "We can do this by minimizing the cost function. Mean squared error is a convex objective function, therefore we can compute its minimum using calculus. Please see video or appendix for this derivation! After computing the minimum, we find that:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat\\theta = \\frac{\\vec{x}^\\top \\vec{y}}{\\vec{x}^\\top \\vec{x}}\n",
        "\\end{align}\n",
        "\n",
        "This is known as solving the normal equations. For different ways of obtaining the solution, see the notes on [Least Squares Optimization](https://www.cns.nyu.edu/~eero/NOTES/leastSquares.pdf) by Eero Simoncelli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNXOyhhX6zDP"
      },
      "source": [
        "### Exercise 2: Solve for the Optimal Estimator\n",
        "\n",
        "In this exercise, you have to write a function that finds the optimal $\\hat{\\theta}$ value using the least squares optimization approach (the equation above) to solve MSE minimization. It shoud take arguments $x$ and $y$ and return the solution $\\hat{\\theta}$.\n",
        "\n",
        "We will then use your function to compute $\\hat{\\theta}$ and plot the resulting prediction on top of the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWv9XClk6zDP"
      },
      "source": [
        "def solve_normal_eqn(x, y):\n",
        "  \"\"\"Solve the normal equations to produce the value of theta_hat that minimizes\n",
        "    MSE.\n",
        "\n",
        "    Args:\n",
        "    x (ndarray): An array of shape (samples,) that contains the input values.\n",
        "    y (ndarray): An array of shape (samples,) that contains the corresponding\n",
        "      measurement values to the inputs.\n",
        "\n",
        "  Returns:\n",
        "    float: the value for theta_hat arrived from minimizing MSE\n",
        "  \"\"\"\n",
        "\n",
        "  ################################################################################\n",
        "  ## TODO for students: solve for the best parameter using least squares\n",
        "  # Fill out function and remove the next line by commenting out.\n",
        "  raise NotImplementedError(\"Student exercise: solve for theta_hat using least squares\")\n",
        "  ################################################################################\n",
        "\n",
        "  # Compute theta_hat analytically\n",
        "  theta_hat = ...\n",
        "\n",
        "  return theta_hat\n",
        "\n",
        "\n",
        "# Uncomment below to test your function\n",
        "#theta_hat = solve_normal_eqn(x, y)\n",
        "#y_hat = theta_hat * x\n",
        "#plot_observed_vs_predicted(x, y, y_hat, theta_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "both",
        "outputId": "5a46f798-31e1-4fbd-da7a-de8a270c5f7d",
        "id": "Pud5Vjaz6zDQ"
      },
      "source": [
        "\n",
        "\n",
        "*Example output:*\n",
        "![Solution Hint](https://drive.google.com/uc?export=view&id=1wSQyRMyHNKFSTaWtZdMxfDte3T_djcoj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhsFjt8nFePg"
      },
      "source": [
        "Now you see how good our model is at estimating $\\hat{\\theta}$. Its accurate upto 1 decimal places using least square optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRxJbvF9Lcrf"
      },
      "source": [
        "---\n",
        "# Section 3: Multiple Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIa4EddEbGvG",
        "cellView": "form"
      },
      "source": [
        "#@title Video 2: Multiple Linear Regression and Polynomial Regression\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"d4nfTki6Ejc\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXzjcn-RLcrg"
      },
      "source": [
        "Now that we have considered the univariate case and how to produce confidence intervals for our estimator, we turn to the general linear regression case, where we can have more than one regressor, or feature, in our input.\n",
        "\n",
        "Recall that our original univariate linear model was given as\n",
        "\n",
        "\\begin{align}\n",
        "y = \\theta x + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "where $\\theta$ is the slope and $\\epsilon$ some noise. We can easily extend this to the multivariate scenario by adding another parameter for each additional feature\n",
        "\n",
        "\\begin{align}\n",
        "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... +\\theta_d x_d + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "where $\\theta_0$ is the intercept and $d$ is the number of features (it is also the dimensionality of our input).\n",
        "\n",
        "We can condense this succinctly using vector notation for a single data point\n",
        "\n",
        "\\begin{align}\n",
        "y_i = \\boldsymbol{\\theta}^{\\top}\\mathbf{x}_i + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "and fully in matrix form\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{\\epsilon}\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf{y}$ is a vector of measurements, $\\mathbf{X}$ is a matrix containing the feature values (columns) for each input sample (rows), and $\\boldsymbol{\\theta}$ is our parameter vector.\n",
        "\n",
        "This matrix $\\mathbf{X}$ is often referred to as the \"[design matrix](https://en.wikipedia.org/wiki/Design_matrix)\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGOV7KmzLcri"
      },
      "source": [
        "For this tutorial we will focus on the two-dimensional case ($d=2$), which allows us to easily visualize our results. As an example, think of a situation where a scientist records the spiking response of a retinal ganglion cell to patterns of light signals that vary in contrast and in orientation. Then contrast and orientation values can be used as features / regressors to predict the cells response.\n",
        "\n",
        "In this case our model can be writen as:\n",
        "\n",
        "\\begin{align}\n",
        "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "or in matrix form where\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{X} = \n",
        "\\begin{bmatrix}\n",
        "1 & x_{1,1} & x_{1,2} \\\\\n",
        "1 & x_{2,1} & x_{2,2} \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "1 & x_{n,1} & x_{n,2}\n",
        "\\end{bmatrix}, \n",
        "\\boldsymbol{\\theta} =\n",
        "\\begin{bmatrix}\n",
        "\\theta_0 \\\\\n",
        "\\theta_1 \\\\\n",
        "\\theta_2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "For our actual exploration dataset we shall set $\\boldsymbol{\\theta}=[0, -2, -3]$ and draw $N=40$ noisy samples from $x \\in [-2,2)$. Note that setting the value of $\\theta_0 = 0$ effectively ignores the offset term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UHzA5U4Lcrj"
      },
      "source": [
        "# Execute this cell to simulate some data\n",
        "# Running this code is necessary for following exercise 3\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Set parameters\n",
        "theta = [0, -2, -3]\n",
        "n_samples = 40\n",
        "\n",
        "# Draw x and calculate y\n",
        "n_regressors = len(theta)\n",
        "x0 = np.ones((n_samples, 1))\n",
        "x1 = np.random.uniform(-2, 2, (n_samples, 1))\n",
        "x2 = np.random.uniform(-2, 2, (n_samples, 1))\n",
        "X = np.hstack((x0, x1, x2))\n",
        "noise = np.random.randn(n_samples)\n",
        "y = X @ theta + noise\n",
        "\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.plot(X[:,1], X[:,2], y, '.')\n",
        "\n",
        "ax.set(\n",
        "    xlabel='$x_1$',\n",
        "    ylabel='$x_2$',\n",
        "    zlabel='y'\n",
        ")\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZOCRt-fLcrk"
      },
      "source": [
        "Now that we have our dataset, we want to find an optimal vector of paramters $\\boldsymbol{\\hat\\theta}$. Recall our analytic solution to minimizing MSE for a single regressor:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat\\theta = \\frac{\\sum_{i=1}^N x_i y_i}{\\sum_{i=1}^N x_i^2}.\n",
        "\\end{align}\n",
        "\n",
        "The same holds true for the multiple regressor case, only now expressed in matrix form\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{\\hat\\theta} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}.\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvJSVuCLLcrm"
      },
      "source": [
        "### Exercise 3: Ordinary Least Squares Estimator\n",
        "\n",
        "In this exercise you will implement the OLS approach to estimating $\\boldsymbol{\\hat\\theta}$ from the design matrix $\\mathbf{X}$ and measurement vector $\\mathbf{y}$. You can use the `@` symbol for matrix multiplication, `.T` for transpose, and `np.linalg.inv` for matrix inversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Um7BOvLcrm"
      },
      "source": [
        "def ordinary_least_squares(X, y):\n",
        "  \"\"\"Ordinary least squares estimator for linear regression.\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): design matrix of shape (n_samples, n_regressors)\n",
        "    y (ndarray): vector of measurements of shape (n_samples)\n",
        "\n",
        "  Returns:\n",
        "    ndarray: estimated parameter values of shape (n_regressors)\n",
        "  \"\"\"\n",
        "  ######################################################################\n",
        "  ## TODO for students: solve for the optimal parameter vector using OLS\n",
        "  # Fill out function and remove the next line by commenting out\n",
        "  raise NotImplementedError(\"Student exercise: solve for theta_hat vector using OLS\")\n",
        "  ######################################################################\n",
        "\n",
        "  # Compute theta_hat using OLS\n",
        "  theta_hat = ...\n",
        "\n",
        "  return theta_hat\n",
        "\n",
        "\n",
        "# Uncomment below to test your function\n",
        "#theta_hat = ordinary_least_squares(X, y)\n",
        "#print(theta_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK0VeDiJLcrn"
      },
      "source": [
        "After filling in this function, you should see that $\\hat{\\theta}$ = [ 0.13861386, -2.09395731, -3.16370742]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-R7Cvt-Lcro"
      },
      "source": [
        "Now that we have our $\\mathbf{\\hat\\theta}$, we can obtain $\\mathbf{\\hat y}$ and thus our mean squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO0LisnBLHPS"
      },
      "source": [
        "## Exercise 4: Compute y_hat and MSE\r\n",
        "\r\n",
        "In this one you have to compute y_hat for the given input X and theta_hat obtained from above. Then you have to again follow the same procedure as in exercise 1 to obtain MSE and make sure to print and match it with the expected value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-q4TJcpLcro"
      },
      "source": [
        "# Compute predicted data\n",
        "theta_hat = ordinary_least_squares(X, y)\n",
        "y_hat = ...\n",
        "# Compute MSE remove \n",
        "print(f\"MSE = {'''....''':.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqxMU8Jo-sQS"
      },
      "source": [
        "The MSE should come out to be 0.91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X41s12NLcrp"
      },
      "source": [
        "Finally, the following code will plot a geometric visualization of the data points (blue) and fitted plane. Execute the following cell to visualize data and predicted plane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPe5xVNOLcrp"
      },
      "source": [
        "# Execute this cell to visualize data and predicted plane\n",
        "\n",
        "theta_hat = ordinary_least_squares(X, y)\n",
        "xx, yy = np.mgrid[-2:2:50j, -2:2:50j]\n",
        "y_hat_grid = np.array([xx.flatten(), yy.flatten()]).T @ theta_hat[1:]\n",
        "y_hat_grid = y_hat_grid.reshape((50, 50))\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.plot(X[:, 1], X[:, 2], y, '.')\n",
        "ax.plot_surface(xx, yy, y_hat_grid, linewidth=0, alpha=0.5, color='C1',\n",
        "                cmap=plt.get_cmap('coolwarm'))\n",
        "\n",
        "for i in range(len(X)):\n",
        "  ax.plot((X[i, 1], X[i, 1]),\n",
        "          (X[i, 2], X[i, 2]),\n",
        "          (y[i], y_hat[i]),\n",
        "          'g-', alpha=.5)\n",
        "\n",
        "ax.set(\n",
        "    xlabel='$x_1$',\n",
        "    ylabel='$x_2$',\n",
        "    zlabel='y'\n",
        ")\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUflXBJiZEd"
      },
      "source": [
        "---\n",
        "# Section 4:  Polynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFp1VvgoiZEe"
      },
      "source": [
        "So far today, you learned how to predict outputs from inputs by fitting a linear regression model. We can now model all sort of relationships, including in neuroscience!  \n",
        "\n",
        "One potential problem with this approach is the simplicity of the model. Linear regression, as the name implies, can only capture a linear relationship between the inputs and outputs. Put another way, the predicted outputs are only a weighted sum of the inputs. What if there are more complicated computations happening? Luckily, many more complex models exist (and you will encounter many more over the next 3 weeks). One model that is still very simple to fit and understand, but captures more complex relationships, is **polynomial regression**, an extension of linear regression.\n",
        "\n",
        "Since polynomial regression is an extension of linear regression, everything you learned so far will come in handy now! The goal is the same: we want to predict the dependent variable $y_{n}$ given the input values $x_{n}$. The key change is the type of relationship between inputs and outputs that the model can capture. \n",
        "\n",
        "Linear regression models predict the outputs as a weighted sum of the inputs:\n",
        "\n",
        "$$y_{n}= \\theta_0 + \\theta x_{n} + \\epsilon_{n}$$\n",
        "\n",
        "With polynomial regression, we model the outputs as a polynomial equation based on the inputs. For example, we can model the outputs as:\n",
        "\n",
        "$$y_{n}= \\theta_0 + \\theta_1 x_{n} + \\theta_2 x_{n}^2 + \\theta_3 x_{n}^3 + \\epsilon_{n}$$\n",
        "\n",
        "We can change how complex a polynomial is fit by changing the order of the polynomial. The order of a polynomial refers to the highest power in the polynomial. The equation above is a third order polynomial because the highest value x is raised to is 3. We could add another term ($+ \\theta_4 x_{n}^4$) to model an order 4 polynomial and so on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIoMpXHHiZEf"
      },
      "source": [
        "First, we will simulate some data to practice fitting polynomial regression models. We will generate random inputs $x$ and then compute y according to $y = x^2 - x - 2 $, with some extra noise both in the input and the output to make the model fitting exercise closer to a real life situation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TIhFZoI9iZEf"
      },
      "source": [
        "#@title\n",
        "#@markdown Execute this cell to simulate some data\n",
        "\n",
        "# setting a fixed seed to our random number generator ensures we will always\n",
        "# get the same psuedorandom number sequence\n",
        "np.random.seed(121)\n",
        "n_samples = 30\n",
        "x = np.random.uniform(-2, 2.5, n_samples)  # inputs uniformly sampled from [-2, 2.5)\n",
        "y =  x**2 - x - 2   # computing the outputs\n",
        "\n",
        "output_noise = 1/8 * np.random.randn(n_samples)\n",
        "y += output_noise  # adding some output noise\n",
        "\n",
        "input_noise = 1/2 * np.random.randn(n_samples)\n",
        "x += input_noise  # adding some input noise\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y)  # produces a scatter plot\n",
        "ax.set(xlabel='x', ylabel='y');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fZMbKIeGEhp4"
      },
      "source": [
        "#@title Helper Functions\r\n",
        "\r\n",
        "def plot_fitted_polynomials(x, y, theta_hat):\r\n",
        "  \"\"\" Plot polynomials of different orders\r\n",
        "\r\n",
        "  Args:\r\n",
        "    x (ndarray): input vector of shape (n_samples)\r\n",
        "    y (ndarray): vector of measurements of shape (n_samples)\r\n",
        "    theta_hat (dict): polynomial regression weights for different orders\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  x_grid = np.linspace(x.min() - .5, x.max() + .5)\r\n",
        "\r\n",
        "  plt.figure()\r\n",
        "\r\n",
        "  for order in range(0, max_order + 1):\r\n",
        "    X_design = make_design_matrix(x_grid, order)\r\n",
        "    plt.plot(x_grid, X_design @ theta_hat[order]);\r\n",
        "\r\n",
        "  plt.ylabel('y')\r\n",
        "  plt.xlabel('x')\r\n",
        "  plt.plot(x, y, 'C0.');\r\n",
        "  plt.legend([f'order {o}' for o in range(max_order + 1)], loc=1)\r\n",
        "  plt.title('polynomial fits')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VniylWhiZEg"
      },
      "source": [
        "## Section 4.1: Design matrix for polynomial regression\n",
        "\n",
        "Now we have the basic idea of polynomial regression and some noisy data, let's begin! The key difference between fitting a linear regression model and a polynomial regression model lies in how we structure the input variables.  \n",
        "\n",
        "For linear regression, we used $X = x$ as the input data. To add a constant bias (a y-intercept in a 2-D plot), we use $X = \\big[ \\boldsymbol 1, x \\big]$, where $\\boldsymbol 1$ is a column of ones.  When fitting, we learn a weight for each column of this matrix. So we learn a weight that multiples with column 1 - in this case that column is all ones so we gain the bias parameter ($+ \\theta_0$). We also learn a weight for every column, or every feature of x, as we learned in Section 1.\n",
        "\n",
        "This matrix $X$ that we use for our inputs is known as a **design matrix**. We want to create our design matrix so we learn weights for $x^2, x^3,$ etc. Thus, we want to build our design matrix $X$ for polynomial regression of order $k$ as:\n",
        "\n",
        "$$X = \\big[ \\boldsymbol 1 , x^1, x^2 , \\ldots , x^k \\big],$$\n",
        "\n",
        "where $\\boldsymbol{1}$ is the vector the same length as $x$ consisting of of all ones, and $x^p$ is the vector or matrix $x$ with all elements raised to the power $p$. Note that $\\boldsymbol{1} = x^0$ and $x^1 = x$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KFmA0XOiZEg"
      },
      "source": [
        "### Exercise 5: Structure design matrix\n",
        "\n",
        "Create a function (`make_design_matrix`) that structures the design matrix given the input data and the order of the polynomial you wish to fit. We will print part of this design matrix for our data and order 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyYXKLuFiZEh"
      },
      "source": [
        "def make_design_matrix(x, order):\n",
        "  \"\"\"Create the design matrix of inputs for use in polynomial regression\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): input vector of shape (n_samples)\n",
        "    order (scalar): polynomial regression order\n",
        "\n",
        "  Returns:\n",
        "    ndarray: design matrix for polynomial regression of shape (samples, order+1)\n",
        "  \"\"\"\n",
        "  ########################################################################\n",
        "  ## TODO for students: create the design matrix ##\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: create the design matrix\")\n",
        "  ########################################################################\n",
        "\n",
        "  # Broadcast to shape (n x 1) so dimensions work\n",
        "  if x.ndim == 1:\n",
        "    x = x[:, None]\n",
        "\n",
        "  #if x has more than one feature, we don't want multiple columns of ones so we assign\n",
        "  # x^0 here\n",
        "  design_matrix = np.ones((x.shape[0], 1))\n",
        "\n",
        "  # Loop through rest of degrees and stack columns (hint: np.hstack)\n",
        "  for degree in range(1, order + 1):\n",
        "      design_matrix = ...\n",
        "\n",
        "  return design_matrix\n",
        "\n",
        "\n",
        "# Uncomment to test your function\n",
        "order = 5\n",
        "# X_design = make_design_matrix(x, order)\n",
        "# print(X_design[0:2, 0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xu1BYUniZEh"
      },
      "source": [
        "You should see that the printed section of this design matrix is `[[ 1.         -1.51194917]\n",
        " [ 1.         -0.35259945]]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4m-Nrv6iZEi"
      },
      "source": [
        "## Section 4.2: Fitting polynomial regression models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6sbgiNAiZEi"
      },
      "source": [
        "Now that we have the inputs structured correctly in our design matrix, fitting a polynomial regression is the same as fitting a linear regression model! All of the polynomial structure we need to learn is contained in how the inputs are structured in the design matrix. We can use the same least squares solution we computed in previous exercises. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAv7ijE1iZEi"
      },
      "source": [
        "### Exercise 6: Fitting polynomial regression models with different orders "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4oa-3ZtiZEj"
      },
      "source": [
        "Here, we will fit polynomial regression models to find the regression coefficients ($\\theta_0, \\theta_1, \\theta_2,$ ...) by solving the least squares problem. Create a function `solve_poly_reg` that loops over different order polynomials (up to `max_order`), fits that model, and saves out the weights for each. You may invoke the `ordinary_least_squares` function. \n",
        "\n",
        "We will then qualitatively inspect the quality of our fits for each order by plotting the fitted polynomials on top of the data. In order to see smooth curves, we evaluate the fitted polynomials on a grid of $x$ values (ranging between the largest and smallest of the inputs present in the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elmr54n1iZEj"
      },
      "source": [
        "def solve_poly_reg(x, y, max_order):\n",
        "  \"\"\"Fit a polynomial regression model for each order 0 through max_order.\n",
        "\n",
        "  Args:\n",
        "    x (ndarray): input vector of shape (n_samples)\n",
        "    y (ndarray): vector of measurements of shape (n_samples)\n",
        "    max_order (scalar): max order for polynomial fits\n",
        "\n",
        "  Returns:\n",
        "    dict: fitted weights for each polynomial model (dict key is order)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a dictionary with polynomial order as keys,\n",
        "  # and np array of theta_hat (weights) as the values\n",
        "  theta_hats = {}\n",
        "\n",
        "  # Loop over polynomial orders from 0 through max_order\n",
        "  for order in range(max_order + 1):\n",
        "\n",
        "    ##################################################################################\n",
        "    ## TODO for students: Create design matrix and fit polynomial model for this order\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Student exercise: fit a polynomial model\")\n",
        "    ##################################################################################\n",
        "\n",
        "    # Create design matrix\n",
        "    X_design = ...\n",
        "\n",
        "    # Fit polynomial model\n",
        "    this_theta = ...\n",
        "\n",
        "    theta_hats[order] = this_theta\n",
        "\n",
        "  return theta_hats\n",
        "\n",
        "\n",
        "# Uncomment to test your function\n",
        "max_order = 5\n",
        "# theta_hats = solve_poly_reg(x, y, max_order)\n",
        "# plot_fitted_polynomials(x, y, theta_hats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812RkzdQiZEl"
      },
      "source": [
        "## Section 4.3: Evaluating fit quality\n",
        "\n",
        "As with linear regression, we can compute mean squared error (MSE) to get a sense of how well the model fits the data. \n",
        "\n",
        "We compute MSE as:\n",
        "\n",
        "$$ MSE = \\frac 1 N ||y - \\hat y||^2 = \\frac 1 N \\sum_{i=1}^N (y_i - \\hat y_i)^2 $$\n",
        "\n",
        "where the predicted values for each model are given by $ \\hat y = X \\hat \\theta$. \n",
        "\n",
        "*Which model (i.e. which polynomial order) do you think will have the best MSE?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCG0wFQiZEl"
      },
      "source": [
        "### Exercise 7: Compute MSE and compare models\n",
        "\n",
        "We will compare the MSE for different polynomial orders with a bar plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_VJQOZtiZEm"
      },
      "source": [
        "mse_list = []\n",
        "order_list = list(range(max_order + 1))\n",
        "\n",
        "for order in order_list:\n",
        "\n",
        "  X_design = make_design_matrix(x, order)\n",
        "\n",
        "  ############################################################################\n",
        "  # TODO for students: Compute MSE (fill in ... sections)\n",
        "  #############################################################################\n",
        "\n",
        "  # Get prediction for the polynomial regression model of this order\n",
        "  y_hat = ...\n",
        "\n",
        "  # Compute the residuals\n",
        "  residuals = ...\n",
        "\n",
        "  # Compute the MSE\n",
        "  mse = ...\n",
        "\n",
        "  mse_list.append(mse)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Uncomment once above exercise is complete\n",
        "# ax.bar(order_list, mse_list)\n",
        "\n",
        "ax.set(title='Comparing Polynomial Fits', xlabel='Polynomial order', ylabel='MSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp7OHmrScRYY"
      },
      "source": [
        "---\n",
        "# Section 5: Bias-variance tradeoff\n",
        "\n",
        "Finding a good model can be difficult. One of the most important concepts to keep in mind when modeling is the **bias-variance tradeoff**. \n",
        "\n",
        "**Bias** is the difference between the prediction of the model and the corresponding true output variables you are trying to predict. Models with high bias will not fit the training data well since the predictions are quite different from the true data. These high bias models are overly simplified - they do not have enough parameters and complexity to accurately capture the patterns in the data and are thus **underfitting**.\n",
        "\n",
        "\n",
        "**Variance** refers to the variability of model predictions for a given input. Essentially, do the model predictions change a lot with changes in the exact training data used? Models with high variance are highly dependent on the exact training data used - they will not generalize well to test data. These high variance models are **overfitting** to the data.\n",
        "\n",
        "In essence:\n",
        "\n",
        "* High bias, low variance models have high train and test error.\n",
        "* Low bias, high variance models have low train error, high test error\n",
        "* Low bias, low variance models have low train and test error\n",
        "\n",
        "\n",
        "As we can see from this list, we ideally want low bias and low variance models! These goals can be in conflict though - models with enough complexity to have low bias also tend to overfit and depend on the training data more. We need to decide on the correct tradeoff.\n",
        "\n",
        "In this section, we will see the bias-variance tradeoff in action with polynomial regression models of different orders.\n",
        "\n",
        "Graphical illustration of bias and variance.\n",
        "(Source: http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
        "\n",
        "![bias-variance](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qfA4F2NKcRYP"
      },
      "source": [
        "#@title Video 3: Bias Variance Tradeoff\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"NcUH_seBcVw\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6wQpxDGIcRYX"
      },
      "source": [
        "#@title\n",
        "#@markdown Execute this cell to simulate both training and test data\n",
        "\n",
        "### Generate training data\n",
        "np.random.seed(0)\n",
        "n_train_samples = 50\n",
        "x_train = np.random.uniform(-2, 2.5, n_train_samples) # sample from a uniform distribution over [-2, 2.5)\n",
        "noise = np.random.randn(n_train_samples) # sample from a standard normal distribution\n",
        "y_train =  x_train**2 - x_train - 2 + noise\n",
        "\n",
        "### Generate testing data\n",
        "n_test_samples = 20\n",
        "x_test = np.random.uniform(-3, 3, n_test_samples) # sample from a uniform distribution over [-2, 2.5)\n",
        "noise = np.random.randn(n_test_samples) # sample from a standard normal distribution\n",
        "y_test =  x_test**2 - x_test - 2 + noise\n",
        "\n",
        "## Plot both train and test data\n",
        "fig, ax = plt.subplots()\n",
        "plt.title('Training & Test Data')\n",
        "plt.plot(x_train, y_train, '.', markersize=15, label='Training')\n",
        "plt.plot(x_test, y_test, 'g+', markersize=15, label='Test')\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "taRFZ_K0cRYa"
      },
      "source": [
        "#@title\n",
        "#@markdown Execute this cell to estimate theta_hats\n",
        "max_order = 5\n",
        "theta_hats = solve_poly_reg(x_train, y_train, max_order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7SLgIN0cRYb"
      },
      "source": [
        "### Exercise 8: Compute and compare train vs test error\n",
        "\n",
        "We will use MSE as our error metric again. Compute MSE on training data ($x_{train},y_{train}$) and test data ($x_{test}, y_{test}$) for each polynomial regression model (orders 0-5). Since you already developed code in T4 Exercise 4 for evaluating fit polynomials, we have ported that here into the function ``evaluate_poly_reg`` for your use.\n",
        "\n",
        "*Please think about after completing exercise before reading the following text! Do you think the order 0 model has high or low bias? High or low variance? How about the order 5 model?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaQfu1EdcRYc"
      },
      "source": [
        "def evaluate_poly_reg(x, y, theta_hats, max_order):\n",
        "    \"\"\" Evaluates MSE of polynomial regression models on data\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): input vector of shape (n_samples)\n",
        "      y (ndarray): vector of measurements of shape (n_samples)\n",
        "      theta_hats (dict):  fitted weights for each polynomial model (dict key is order)\n",
        "      max_order (scalar): max order of polynomial fit\n",
        "\n",
        "    Returns\n",
        "      (ndarray): mean squared error for each order, shape (max_order)\n",
        "    \"\"\"\n",
        "\n",
        "    mse = np.zeros((max_order + 1))\n",
        "    for order in range(0, max_order + 1):\n",
        "      X_design = make_design_matrix(x, order)\n",
        "      y_hat = np.dot(X_design, theta_hats[order])\n",
        "      residuals = y - y_hat\n",
        "      mse[order] = np.mean(residuals ** 2)\n",
        "\n",
        "    return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIZCK0THcRYc"
      },
      "source": [
        "def compute_mse(x_train,x_test,y_train,y_test,theta_hats,max_order):\n",
        "  \"\"\"Compute MSE on training data and test data.\n",
        "\n",
        "  Args:\n",
        "    x_train(ndarray): training data input vector of shape (n_samples)\n",
        "    x_test(ndarray): test data input vector of shape (n_samples)\n",
        "    y_train(ndarray): training vector of measurements of shape (n_samples)\n",
        "    y_test(ndarray): test vector of measurements of shape (n_samples)\n",
        "    theta_hats(dict): fitted weights for each polynomial model (dict key is order)\n",
        "    max_order (scalar): max order of polynomial fit\n",
        "\n",
        "  Returns:\n",
        "    ndarray, ndarray: MSE error on training data and test data for each order\n",
        "  \"\"\"\n",
        "\n",
        "  #######################################################\n",
        "  ## TODO for students: calculate mse error for both sets\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: calculate mse for train and test set\")\n",
        "  #######################################################\n",
        "\n",
        "  mse_train = ...\n",
        "  mse_test = ...\n",
        "\n",
        "  return mse_train, mse_test\n",
        "\n",
        "\n",
        "#Uncomment below to test your function\n",
        "# mse_train, mse_test = compute_mse(x_train, x_test, y_train, y_test, theta_hats, max_order)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "width = .35\n",
        "\n",
        "# ax.bar(np.arange(max_order + 1) - width / 2, mse_train, width, label=\"train MSE\")\n",
        "# ax.bar(np.arange(max_order + 1) + width / 2, mse_test , width, label=\"test MSE\")\n",
        "\n",
        "ax.legend()\n",
        "ax.set(xlabel='Polynomial order', ylabel='MSE', title ='Comparing polynomial fits');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O-Vh9n4DKi_"
      },
      "source": [
        "As we can see from the plot above, more complex models (higher order polynomials) have lower MSE for training data. The overly simplified models (orders 0 and 1) have high MSE on the training data. As we add complexity to the model, we go from high bias to low bias.\r\n",
        "\r\n",
        "The MSE on test data follows a different pattern. The best test MSE is for an order 2 model - this makes sense as the data was generated with an order 2 model. Both simpler models and more complex models have higher test MSE.\r\n",
        "\r\n",
        "So to recap:\r\n",
        "\r\n",
        "Order 0 model: High bias, low variance\r\n",
        "\r\n",
        "Order 5 model: Low bias, high variance\r\n",
        "\r\n",
        "Order 2 model: Just right, low bias, low variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5xDhDQm6zDS"
      },
      "source": [
        "---\n",
        "# Congratulations!\n",
        "\n",
        "You have reached to the end of the tutorial. Yes, that was it. However, model fitting isn't limited to Linear Regression. Its very broad and seemingly endless. There are times when one model works in a particular scenario but fails for the other one, we have to tweak the parameters, optimize our algorithm and various other things to reach to our desired accuracy in predictions. <br> So, this was just an overview of how model fits into the input dataset, learns some parameters by itself and predict even for unseen inputs.\\\n",
        "All Credits to NeuroMatch Academy for such a nice piece of work.<br>\n",
        "Hope, you enjoyed and learnt from this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua15XVJjqs2m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}