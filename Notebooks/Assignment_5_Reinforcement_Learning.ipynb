{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BCS Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0AbOn-ATgRS"
      },
      "source": [
        "# BCS Workshop 2020\n",
        "# Reinforcement Learning\n",
        "\n",
        "__Adapted by:__ Debaditya Bhattacharya from NMA Summer School 2020.   \n",
        "\n",
        "## Note\n",
        "\n",
        "This notebook has been divided into 4 parts, and each part has sections, subsections, videos, demos and / or exercises under it. The contents of the notebook are _inherently_ difficult. Watch the videos, and rewatch them if you have to. Google things you don't understand. Seek simple explanations. Take your time while solving the questions. If you feel frustrated or stuck for long, take a break and come back to it a little bit later. Still stuck? Ask a friend or the workshop organizers. \n",
        "\n",
        "Now without further ado, lets get started :)\n",
        "\n",
        "All the best!\n",
        "\n",
        "P.S. Don't forget to copy it to your own drive before you begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogUyHbGxTgRT"
      },
      "source": [
        "---\n",
        "# Part 1\n",
        "## Objectives\n",
        "  \n",
        "In this part, we will learn how to estimate state-value functions in a classical conditioning paradigm using Temporal Difference (TD) learning and examine TD-errors at the presentation of the conditioned and unconditioned stimulus (CS and US) under different CS-US contingencies. These exercises will provide you with an understanding of both how reward prediction errors (RPEs) behave in classical conditioning and what we should expect to see if Dopamine represents a \"canonical\" model-free RPE. \n",
        "\n",
        "At the end of this part:    \n",
        "* You will learn to use the standard tapped delay line conditioning model\n",
        "* You will understand how RPEs move to CS\n",
        "* You will understand how variability in reward size effects RPEs\n",
        "* You will understand how differences in US-CS timing effect RPEs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "ONJe3ZrzTgRU"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2SUgjIsQTgRV"
      },
      "source": [
        "#@title Figure settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6y7sq_KnTgRV"
      },
      "source": [
        "# @title Helper functions\n",
        "from matplotlib import ticker\n",
        "\n",
        "def plot_value_function(V, ax=None, show=True):\n",
        "  \"\"\"Plot V(s), the value function\"\"\"\n",
        "  if not ax:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  ax.stem(V, use_line_collection=True)\n",
        "  ax.set_ylabel('Value')\n",
        "  ax.set_xlabel('State')\n",
        "  ax.set_title(\"Value function: $V(s)$\")\n",
        "\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "def plot_tde_trace(TDE, ax=None, show=True, skip=400):\n",
        "  \"\"\"Plot the TD Error across trials\"\"\"\n",
        "  if not ax:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  indx = np.arange(0, TDE.shape[1], skip)\n",
        "  im = ax.imshow(TDE[:,indx])\n",
        "  positions = ax.get_xticks()\n",
        "  # Avoid warning when setting string tick labels\n",
        "  ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "  ax.set_xticklabels([f\"{int(skip * x)}\" for x in positions])\n",
        "  ax.set_title('TD-error over learning')\n",
        "  ax.set_ylabel('State')\n",
        "  ax.set_xlabel('Iterations')\n",
        "  ax.figure.colorbar(im)\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "def learning_summary_plot(V, TDE):\n",
        "  \"\"\"Summary plot for Ex1\"\"\"\n",
        "  fig, (ax1, ax2) = plt.subplots(nrows = 2, gridspec_kw={'height_ratios': [1, 2]})\n",
        "\n",
        "  plot_value_function(V, ax=ax1, show=False)\n",
        "  plot_tde_trace(TDE, ax=ax2, show=False)\n",
        "  plt.tight_layout()\n",
        "\n",
        "def reward_guesser_title_hint(r1, r2):\n",
        "  \"\"\"\"Provide a mildly obfuscated hint for a demo.\"\"\"\n",
        "  if (r1==14 and r2==6) or (r1==6 and r2==14):\n",
        "    return \"Technically correct...(the best kind of correct)\"\n",
        "\n",
        "  if  ~(~(r1+r2) ^ 11) - 1 == (6 | 24): # Don't spoil the fun :-)\n",
        "    return \"Congratulations! You solved it!\"\n",
        "\n",
        "  return \"Keep trying....\"\n",
        "\n",
        "#@title Default title text\n",
        "class ClassicalConditioning:\n",
        "\n",
        "    def __init__(self, n_steps, reward_magnitude, reward_time):\n",
        "\n",
        "        # Task variables\n",
        "        self.n_steps = n_steps\n",
        "        self.n_actions = 0\n",
        "        self.cs_time = int(n_steps/4) - 1\n",
        "\n",
        "        # Reward variables\n",
        "        self.reward_state = [0,0]\n",
        "        self.reward_magnitude = None\n",
        "        self.reward_probability = None\n",
        "        self.reward_time = None\n",
        "\n",
        "        self.set_reward(reward_magnitude, reward_time)\n",
        "\n",
        "        # Time step at which the conditioned stimulus is presented\n",
        "\n",
        "        # Create a state dictionary\n",
        "        self._create_state_dictionary()\n",
        "\n",
        "    def set_reward(self, reward_magnitude, reward_time):\n",
        "\n",
        "        \"\"\"\n",
        "        Determine reward state and magnitude of reward\n",
        "        \"\"\"\n",
        "        if reward_time >= self.n_steps - self.cs_time:\n",
        "            self.reward_magnitude = 0\n",
        "\n",
        "        else:\n",
        "            self.reward_magnitude = reward_magnitude\n",
        "            self.reward_state = [1, reward_time]\n",
        "\n",
        "    def get_outcome(self, current_state):\n",
        "\n",
        "        \"\"\"\n",
        "        Determine next state and reward\n",
        "        \"\"\"\n",
        "        # Update state\n",
        "        if current_state < self.n_steps - 1:\n",
        "            next_state = current_state + 1\n",
        "        else:\n",
        "            next_state = 0\n",
        "\n",
        "        # Check for reward\n",
        "        if self.reward_state == self.state_dict[current_state]:\n",
        "            reward = self.reward_magnitude\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return next_state, reward\n",
        "\n",
        "    def _create_state_dictionary(self):\n",
        "\n",
        "        \"\"\"\n",
        "        This dictionary maps number of time steps/ state identities\n",
        "        in each episode to some useful state attributes:\n",
        "\n",
        "        state      - 0 1 2 3 4 5 (cs) 6 7 8 9 10 11 12 ...\n",
        "        is_delay   - 0 0 0 0 0 0 (cs) 1 1 1 1  1  1  1 ...\n",
        "        t_in_delay - 0 0 0 0 0 0 (cs) 1 2 3 4  5  6  7 ...\n",
        "        \"\"\"\n",
        "        d = 0\n",
        "\n",
        "        self.state_dict = {}\n",
        "        for s in range(self.n_steps):\n",
        "            if s <= self.cs_time:\n",
        "                self.state_dict[s] = [0,0]\n",
        "            else:\n",
        "                d += 1 # Time in delay\n",
        "                self.state_dict[s] = [1,d]\n",
        "\n",
        "class MultiRewardCC(ClassicalConditioning):\n",
        "  \"\"\"Classical conditioning paradigm, except that one randomly selected reward,\n",
        "    magnitude, from a list, is delivered of a single fixed reward.\"\"\"\n",
        "  def __init__(self, n_steps, reward_magnitudes, reward_time=None):\n",
        "    \"\"\"\"Build a multi-reward classical conditioning environment\n",
        "      Args:\n",
        "        - nsteps: Maximum number of steps\n",
        "        - reward_magnitudes: LIST of possible reward magnitudes.\n",
        "        - reward_time: Single fixed reward time\n",
        "      Uses numpy global random state.\n",
        "      \"\"\"\n",
        "    super().__init__(n_steps, 1, reward_time)\n",
        "    self.reward_magnitudes = reward_magnitudes\n",
        "\n",
        "  def get_outcome(self, current_state):\n",
        "    next_state, reward = super().get_outcome(current_state)\n",
        "    if reward:\n",
        "      reward=np.random.choice(self.reward_magnitudes)\n",
        "    return next_state, reward\n",
        "\n",
        "\n",
        "class ProbabilisticCC(ClassicalConditioning):\n",
        "  \"\"\"Classical conditioning paradigm, except that rewards are stochastically omitted.\"\"\"\n",
        "  def __init__(self, n_steps, reward_magnitude, reward_time=None, p_reward=0.75):\n",
        "    \"\"\"\"Build a multi-reward classical conditioning environment\n",
        "      Args:\n",
        "        - nsteps: Maximum number of steps\n",
        "        - reward_magnitudes: Reward magnitudes.\n",
        "        - reward_time: Single fixed reward time.\n",
        "        - p_reward: probability that reward is actually delivered in rewarding state\n",
        "      Uses numpy global random state.\n",
        "      \"\"\"\n",
        "    super().__init__(n_steps, reward_magnitude, reward_time)\n",
        "    self.p_reward = p_reward\n",
        "\n",
        "  def get_outcome(self, current_state):\n",
        "    next_state, reward = super().get_outcome(current_state)\n",
        "    if reward:\n",
        "      reward*= int(np.random.uniform(size=1)[0] < self.p_reward)\n",
        "    return next_state, reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tluPcoUcTgRX"
      },
      "source": [
        "---\n",
        "## Section 1: TD-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "cellView": "form",
        "id": "EiZaUUBQTgRY",
        "outputId": "7077d7cd-9e49-4982-e11c-0cd0ec7b06cb"
      },
      "source": [
        "#@title Video 1: Introduction\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"YoNbc9M92YY\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/YoNbc9M92YY\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/YoNbc9M92YY?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f826b8bd390>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zmnqb7RTgRZ"
      },
      "source": [
        "__Environment:__\n",
        "\n",
        "- The agent experiences the environment in episodes or trials. \n",
        "- Episodes terminate by transitioning to the inter-trial-interval (ITI) state and they are initiated from the ITI state as well. We clamp the value of the terminal/ITI states to zero. \n",
        "- The classical conditioning environment is composed of a sequence of states that the agent deterministically transitions through. Starting at State 0, the agent moves to State 1 in the first step, from State 1 to State 2 in the second, and so on.  These states represent time in the tapped delay line representation\n",
        "- Within each episode, the agent is presented a CS and US (reward). \n",
        "- The CS is always presented at 1/4 of the total duration of the trial. The US (reward) is then delivered after the CS. The interval between the CS and US is specified by `reward_time`.\n",
        "- The agent's goal is to learn to predict expected rewards from each state in the trial. \n",
        "\n",
        "\n",
        "**General concepts**\n",
        "\n",
        "* Return $G_{t}$: future cumulative reward, which can be written in arecursive form\n",
        "\\begin{align}\n",
        "G_{t} &= \\sum \\limits_{k = 0}^{\\infty} \\gamma^{k} r_{t+k+1} \\\\\n",
        "&= r_{t+1} + \\gamma G_{t+1}\n",
        "\\end{align}\n",
        "where $\\gamma$ is discount factor that controls the importance of future rewards, and $\\gamma \\in [0, 1]$. $\\gamma$ may also be interpreted as probability of continuing the trajectory.\n",
        "* Value funtion $V_{\\pi}(s_t=s)$: expecation of the return\n",
        "\\begin{align}\n",
        "V_{\\pi}(s_t=s) &= \\mathbb{E} [ G_{t}\\; | \\; s_t=s, a_{t:\\infty}\\sim\\pi] \\\\\n",
        "& = \\mathbb{E} [ r_{t+1} + \\gamma G_{t+1}\\; | \\; s_t=s, a_{t:\\infty}\\sim\\pi]\n",
        "\\end{align}\n",
        "With an assumption of **Markov process**, we thus have:\n",
        "\\begin{align}\n",
        "V_{\\pi}(s_t=s) &= \\mathbb{E} [ r_{t+1} + \\gamma V_{\\pi}(s_{t+1})\\; | \\; s_t=s, a_{t:\\infty}\\sim\\pi] \\\\\n",
        "&= \\sum_a \\pi(a|s) \\sum_{r, s'}p(s', r)(r + V_{\\pi}(s_{t+1}=s'))\n",
        "\\end{align}\n",
        "\n",
        "**Temporal difference (TD) learning**\n",
        "\n",
        "* With a Markovian assumption, we can use $V(s_{t+1})$ as an imperfect proxy for the true value $G_{t+1}$ (Monte Carlo bootstrapping), and thus obtain the generalised equation to calculate TD-error:\n",
        "\\begin{align}\n",
        "\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})\n",
        "\\end{align}\n",
        "\n",
        "* Value updated by using the learning rate constant $\\alpha$:\n",
        "\\begin{align}\n",
        "V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}\n",
        "\\end{align}\n",
        "\n",
        "  (Reference: https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html)\n",
        "\n",
        "\n",
        "\n",
        "__Definitions:__\n",
        "\n",
        "* TD-error:\n",
        "\\begin{align}\n",
        "\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})\n",
        "\\end{align}\n",
        "\n",
        "* Value updates:\n",
        "\\begin{align}\n",
        "V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uomozrGTgRZ"
      },
      "source": [
        "### Exercise 1: TD-learning with guaranteed rewards\n",
        "   \n",
        "Implement TD-learning to estimate the state-value function in the classical-conditioning world with guaranteed rewards, with a fixed magnitude, at a fixed delay after the conditioned stimulus, CS. Save TD-errors over learning (i.e., over trials) so we can visualize them afterwards. \n",
        "\n",
        "In order to simulate the effect of the CS, you should only update $V(s_{t})$ during the delay period after CS. This period is indicated by the boolean variable `is_delay`. This can be implemented by multiplying the expression for updating the value function by `is_delay`.\n",
        "\n",
        "Use the provided code to estimate the value function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vph1-MOtTgRb"
      },
      "source": [
        "def td_learner(env, n_trials, gamma=0.98, alpha=0.001):\n",
        "  \"\"\" Temporal Difference learning\n",
        "\n",
        "  Args:\n",
        "    env (object): the environment to be learned\n",
        "    n_trials (int): the number of trials to run\n",
        "    gamma (float): temporal discount factor\n",
        "    alpha (float): learning rate\n",
        "\n",
        "  Returns:\n",
        "    ndarray, ndarray: the value function and temporal difference error arrays\n",
        "  \"\"\"\n",
        "  V = np.zeros(env.n_steps) # Array to store values over states (time)\n",
        "  TDE = np.zeros((env.n_steps, n_trials)) # Array to store TD errors\n",
        "\n",
        "  for n in range(n_trials):\n",
        "    state = 0 # Initial state\n",
        "    for t in range(env.n_steps):\n",
        "      # Get next state and next reward\n",
        "      next_state, reward = env.get_outcome(state)\n",
        "      # Is the current state in the delay period (after CS)?\n",
        "      is_delay = env.state_dict[state][0]\n",
        "\n",
        "      ########################################################################\n",
        "      ## TODO for students: implement TD error and value function update\n",
        "      # Fill out function and remove\n",
        "      raise NotImplementedError(\"Student excercise: implement TD error and value function update\")\n",
        "      #################################################################################\n",
        "      # Write an expression to compute the TD-error\n",
        "      TDE[state, n] = ...\n",
        "\n",
        "      # Write an expression to update the value function\n",
        "      V[state] += ...\n",
        "\n",
        "      # Update state\n",
        "      state = next_state\n",
        "\n",
        "  return V, TDE\n",
        "\n",
        "\n",
        "# Uncomment once the td_learner function is complete\n",
        "# env = ClassicalConditioning(n_steps=40, reward_magnitude=10, reward_time=10)\n",
        "# V, TDE = td_learner(env, n_trials=20000)\n",
        "# learning_summary_plot(V, TDE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "7e947f90-bf43-465c-8090-78b4e2eb41a6",
        "id": "AgLwRD6yTgRb"
      },
      "source": [
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=558 height=414 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial1_Solution_6f2c8b56_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34sQzwAhTgRc"
      },
      "source": [
        "### Interactive Demo 1: US to CS Transfer \n",
        "\n",
        "During classical conditioning, the subject's behavioral response (e.g., salivating) transfers from the unconditioned stimulus (US; like the smell of tasty food) to the conditioned stimulus (CS; like Pavlov ringing his bell) that predicts it. Reward prediction errors play an important role in this process by adjusting the value of states according to their expected, discounted return.\n",
        "\n",
        "Use the widget below to examine how reward prediction errors change over time. Before training (orange line), only the reward state has high reward prediction error. As training progresses (blue line, slider), the reward prediction errors shift to the conditioned stimulus, where they end up when the trial is complete (green line). \n",
        "\n",
        "Dopamine neurons, which are thought to carry reward prediction errors _in vivo_, show exactly the same behavior!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2naHbd8TgRc"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "n_trials = 20000\n",
        "\n",
        "@widgets.interact\n",
        "def plot_tde_by_trial(trial = widgets.IntSlider(value=5000, min=0, max=n_trials-1 , step=1, description=\"Trial #\")):\n",
        "  if 'TDE' not in globals():\n",
        "    print(\"Complete Exercise 1 to enable this interactive demo!\")\n",
        "  else:\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.axhline(0, color='k') # Use this + basefmt=' ' to keep the legend clean.\n",
        "    ax.stem(TDE[:, 0], linefmt='C1-', markerfmt='C1d', basefmt=' ',\n",
        "            label=\"Before Learning (Trial 0)\",\n",
        "            use_line_collection=True)\n",
        "    ax.stem(TDE[:, -1], linefmt='C2-', markerfmt='C2s', basefmt=' ',\n",
        "            label=\"After Learning (Trial $\\infty$)\",\n",
        "            use_line_collection=True)\n",
        "    ax.stem(TDE[:, trial], linefmt='C0-', markerfmt='C0o', basefmt=' ',\n",
        "            label=f\"Trial {trial}\",\n",
        "            use_line_collection=True)\n",
        "\n",
        "    ax.set_xlabel(\"State in trial\")\n",
        "    ax.set_ylabel(\"TD Error\")\n",
        "    ax.set_title(\"Temporal Difference Error by Trial\")\n",
        "    ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHPwCscDTgRd"
      },
      "source": [
        "### Interactive Demo 2: Learning Rates and Discount Factors\n",
        "\n",
        "Our TD-learning agent has two parameters that control how it learns: $\\alpha$, the learning rate, and $\\gamma$, the discount factor. In Exercise 1, we set these parameters to $\\alpha=0.001$ and $\\gamma=0.98$ for you. Here, you'll investigate how changing these parameters alters the model that TD-learning learns.\n",
        "\n",
        "Before enabling the interactive demo below, take a moment to think about the functions of these two parameters. $\\alpha$ controls the size of the Value function updates produced by each TD-error. In our simple, deterministic world, will this affect the final model we learn? Is a larger $\\alpha$ necessarily better in more complex, realistic environments?\n",
        "\n",
        "The discount rate $\\gamma$ applies an exponentially-decaying weight to returns occuring in the future, rather than the present timestep. How does this affect the model we learn? What happens when $\\gamma=0$ or $\\gamma \\geq 1$?\n",
        "\n",
        "Use the widget to test your hypotheses.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1F4S0sEeTgRd"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact\n",
        "def plot_summary_alpha_gamma(alpha = widgets.FloatSlider(value=0.0001, min=0.0001, max=0.1, step=0.0001, readout_format='.4f', description=\"alpha\"),\n",
        "                             gamma = widgets.FloatSlider(value=0.980, min=0, max=1.1, step=0.010, description=\"gamma\")):\n",
        "  env = ClassicalConditioning(n_steps=40, reward_magnitude=10, reward_time=10)\n",
        "  try:\n",
        "    V_params, TDE_params = td_learner(env, n_trials=20000, gamma=gamma, alpha=alpha)\n",
        "  except NotImplementedError:\n",
        "    print(\"Finish Exercise 1 to enable this interactive demo\")\n",
        "\n",
        "  learning_summary_plot(V_params,TDE_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNvDrX8vTgRe"
      },
      "source": [
        "---\n",
        "## Section 2: TD-learning with varying reward magnitudes\n",
        "\n",
        "In the previous exercise, the environment was as simple as possible. On every trial, the CS predicted the same reward, at the same time, with 100% certainty. In the next few exercises, we will make the environment more progressively more complicated and examine the TD-learner's behavior. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONUEYgcbTgRe"
      },
      "source": [
        "### Interactive Demo 3: Match the Value Functions\n",
        "\n",
        "First, will replace the environment with one that dispenses one of several rewards, chosen at random. Shown below is the final value function $V$ for a TD learner that was trained in an enviroment where the CS predicted a reward of 6 or 14 units; both rewards were equally likely). \n",
        "\n",
        "Can you find another pair of rewards that cause the agent to learn the same value function? Assume each reward will be dispensed 50% of the time. \n",
        "\n",
        "Hints:\n",
        "* Carefully consider the definition of the value function $V$. This can be solved analytically.\n",
        "* There is no need to change $\\alpha$ or $\\gamma$. \n",
        "* Due to the randomness, there may be a small amount of variation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "17fFEmkFTgRf"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "n_trials = 20000\n",
        "np.random.seed(2020)\n",
        "rng_state = np.random.get_state()\n",
        "env = MultiRewardCC(40, [6, 14], reward_time=10)\n",
        "V_multi, TDE_multi = td_learner(env, n_trials, gamma=0.98, alpha=0.001)\n",
        "\n",
        "@widgets.interact\n",
        "def reward_guesser_interaction(r1 = widgets.IntText(value=0, min=0, max=50, description=\"Reward 1\"),\n",
        "                               r2 = widgets.IntText(value=0, min=0, max=50, description=\"Reward 2\")):\n",
        "  try:\n",
        "    env2 = MultiRewardCC(40, [r1, r2], reward_time=10)\n",
        "    V_guess, _ = td_learner(env2, n_trials, gamma=0.98, alpha=0.001)\n",
        "    fig, ax = plt.subplots()\n",
        "    m, l, _ = ax.stem(V_multi, linefmt='y-', markerfmt='yo', basefmt=' ', label=\"Target\",\n",
        "            use_line_collection=True)\n",
        "    m.set_markersize(15)\n",
        "    m.set_markerfacecolor('none')\n",
        "    l.set_linewidth(4)\n",
        "    m, _, _ = ax.stem(V_guess, linefmt='r', markerfmt='rx', basefmt=' ', label=\"Guess\",\n",
        "                      use_line_collection=True)\n",
        "    m.set_markersize(15)\n",
        "\n",
        "    ax.set_xlabel(\"State\")\n",
        "    ax.set_ylabel(\"Value\")\n",
        "    ax.set_title(\"Guess V(s)\\n\" + reward_guesser_title_hint(r1, r2))\n",
        "    ax.legend()\n",
        "  except NotImplementedError:\n",
        "    print(\"Please finish Exercise 1 first!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUPkjth-TgRf"
      },
      "source": [
        "### Section 2.1 Examining the TD Error\n",
        "\n",
        "Run the cell below to plot the TD errors from our multi-reward environment. A new feature appears in this plot? What is it? Why does it happen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsCvbCq5TgRf"
      },
      "source": [
        "plot_tde_trace(TDE_multi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiuDyhlnTgRg"
      },
      "source": [
        "---\n",
        "## Section 3: TD-learning with probabilistic rewards\n",
        "\n",
        "In this environment, we'll return to delivering a single reward of ten units. However, it will be delivered intermittently: on 20 percent of trials, the CS will be shown but the agent will not receive the usual reward; the remaining 80% will proceed as usual.\n",
        "\n",
        " Run the cell below to simulate. How does this compare with the previous experiment?\n",
        "\n",
        "Earlier in the notebook, we saw that changing $\\alpha$ had little effect on learning in a deterministic environment. What happens if you set it to an large value, like 1, in this noisier scenario? Does it seem like it will _ever_ converge?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20SPqfNkTgRg"
      },
      "source": [
        "np.random.set_state(rng_state) # Resynchronize everyone's notebooks\n",
        "n_trials = 20000\n",
        "try:\n",
        "  env = ProbabilisticCC(n_steps=40, reward_magnitude=10, reward_time=10,\n",
        "                        p_reward=0.8)\n",
        "  V_stochastic, TDE_stochastic = td_learner(env, n_trials*2, alpha=1)\n",
        "  learning_summary_plot(V_stochastic, TDE_stochastic)\n",
        "except NotImplementedError:\n",
        "  print(\"Please finish Exercise 1 first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB4UaJdmTgRh"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this notebook, we have developed a simple TD Learner and examined how its state representations and reward prediction errors evolve during training. By manipualting its environment and parameters ($\\alpha$, $\\gamma$), you developed an intuition for how it behaves. \n",
        "\n",
        "This simple model closely resembles the behavior of subjects undergoing classical conditioning tasks and the dopamine neurons that may underlie that behavior. You may have implemented TD-reset or used the model to recreate a common experimental error. The update rule used here has been extensively studied for [more than 70 years](https://www.pnas.org/content/108/Supplement_3/15647) as a possible explanation for artificial and biological learning. \n",
        "\n",
        "However, you may have noticed that something is missing from this notebook. We carefully calculated the value of each state, but did not use it to actually do anything. Using values to plan _**Actions**_ is coming up next!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOu3EQQJTgRk"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2\n",
        "## Objectives\n",
        "  \n",
        "In this part you will use 'bandits' to understand the fundementals of how a policy interacts with the learning algorithm in reinforcement learning.\n",
        "    \n",
        "* You will understand the fundemental tradeoff between exploration and exploitation in a policy.\n",
        "* You will understand how the learning rate interacts with exploration to find the best available action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MK963nnBTgRl"
      },
      "source": [
        "#@title Figure settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "N4Odb5uGTgRm"
      },
      "source": [
        "#@title Helper functions\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "def plot_choices(q, epsilon, choice_fn, n_steps=1000, rng_seed=1):\n",
        "  np.random.seed(rng_seed)\n",
        "  counts = np.zeros_like(q)\n",
        "  for t in range(n_steps):\n",
        "    action = choice_fn(q, epsilon)\n",
        "    counts[action] += 1\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(range(len(q)), counts/n_steps)\n",
        "  ax.set(ylabel='% chosen', xlabel='action', ylim=(0,1), xticks=range(len(q)))\n",
        "\n",
        "\n",
        "def plot_multi_armed_bandit_results(results):\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 4))\n",
        "  ax1.plot(results['rewards'])\n",
        "  ax1.set(title=f\"Total Reward: {np.sum(results['rewards']):.2f}\",\n",
        "          xlabel='step', ylabel='reward')\n",
        "  ax2.plot(results['qs'])\n",
        "  ax2.set(xlabel='step', ylabel='value')\n",
        "  ax2.legend(range(len(results['mu'])))\n",
        "  ax3.plot(results['mu'], label='latent')\n",
        "  ax3.plot(results['qs'][-1], label='learned')\n",
        "  ax3.set(xlabel='action', ylabel='value')\n",
        "  ax3.legend()\n",
        "\n",
        "\n",
        "def plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal):\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
        "\n",
        "  ax1.plot(np.mean(trial_rewards, axis=1).T)\n",
        "  ax1.set(title=f'Average Reward ({fixed})', xlabel='step', ylabel='reward')\n",
        "  ax1.legend(labels)\n",
        "\n",
        "  ax2.plot(np.mean(trial_optimal, axis=1).T)\n",
        "  ax2.set(title=f'Performance ({fixed})', xlabel='step', ylabel='% optimal')\n",
        "  ax2.legend(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "borOH-etTgRm"
      },
      "source": [
        "---\n",
        "## Section 1: Multi-Armed Bandits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "-QnCsOo2TgRm",
        "outputId": "d7c855c0-5273-4509-de84-8f88095ae31a"
      },
      "source": [
        "#@title Video 1: Multi-Armed Bandits\n",
        "# Insert the ID of the corresponding youtube video\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"kdiXr1zsfo0\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/kdiXr1zsfo0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/kdiXr1zsfo0?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f826b06ff28>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpmaFDaBTgRn"
      },
      "source": [
        "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a reward signal in the form of a numerical value, where the larger value is the better. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
        "\n",
        "This is the original form of the k-armed bandit problem. This name derives from the colloquial name for a slot machine, the \"one-armed bandit\", because it has the one lever to pull, and it is often rigged to take more money than it pays out over time. The multi-armed bandit extension is to imagine, for instance, that you are faced with multiple slot machines that you can play, but only one at a time. Which machine should you play, i.e. which arm should you pull, which action should you take, at any given time to maximize your total payout.\n",
        "\n",
        "<img alt=\"MultiArmedBandit\" width=\"625\" height=\"269\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial2_MultiarmedBandit.png?raw=true\">\n",
        "\n",
        "\n",
        "While there are many different levels of sophistication and assumptions in how the rewards are determined, for simplicity's sake we will assume that each action results in a reward drawn from a fixed Gaussian distribution with unknown mean and unit variance. This problem setting is referred to as the *environment*, and goal is to find the arm with the highest mean value.\n",
        "\n",
        "We will solve this *optimization problem* with an *agent*, in this case an algorithm that takes in rewards and returns actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL6Sn_kUTgRn"
      },
      "source": [
        "---\n",
        "## Section 2: Choosing an Action\n",
        "   \n",
        "The first thing our agent needs to be able to do is choose which arm to pull. The strategy for choosing actions based on our expectations is called a *policy* (often denoted $\\pi$). We could have a random policy -- just pick an arm at random each time -- though this doesn't seem likely to be capable of optimizing our reward. We want some intentionality, and to do that we need a way of describing our beliefs about the arms' reward potential. We do this with an action-value function\n",
        "\n",
        "\\begin{align}\n",
        "q(a) = \\mathbb{E} [r_{t} | a_{t} = a]\n",
        "\\end{align}\n",
        "\n",
        "where the value $q$ for taking action $a \\in A$ at time $t$ is equal to the expected value of the reward $r_t$ given that we took action $a$ at that time. In practice, this is often represented as an array of values, where each action's value is a different element in the array.\n",
        "\n",
        "Great, now that we have a way to describe our beliefs about the values each action should return, let's come up with a policy.\n",
        "\n",
        "An obvious choice would be to take the action with the highest expected value. This is referred to as the *greedy* policy\n",
        "\n",
        "\\begin{align}\n",
        "a_{t} = \\text{argmax}_{a} \\; q_{t} (a)\n",
        "\\end{align}\n",
        "\n",
        "where our choice action is the one that maximizes the current value function.\n",
        "\n",
        "So far so good, but it can't be this easy. And, in fact, the greedy policy does have a fatal flaw: it easily gets trapped in local maxima. It never explores to see what it hasn't seen before if one option is already better than the others. This leads us to a fundamental challenge in coming up with effective polices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiUpfi4MTgRo"
      },
      "source": [
        "### Section 2.1: The Exploitation-Exploration Dilemma\n",
        "\n",
        "If we never try anything new, if we always stick to the safe bet, we don't know what we are missing. Sometimes we aren't missing much of anything, and regret not sticking with our preferred choice, yet other times we stumble upon something new that was way better than we thought.\n",
        "\n",
        "This is the exploitation-exploration dilemma: do you go with you best choice now, or risk the less certain option with the hope of finding something better. Too much exploration, however, means you may end up with a sub-optimal reward once it's time to stop.\n",
        "\n",
        "In order to avoid getting stuck in local minima while also maximizing reward, effective policies need some way to balance between these two aims.\n",
        "\n",
        "A simple extension to our greedy policy is to add some randomness. For instance, a coin flip -- heads we take the best choice now, tails we pick one at random. This is referred to as the $\\epsilon$-greedy policy:\n",
        "\n",
        "\\begin{align}\n",
        "P (a_{t} = a) = \n",
        "        \\begin{cases}\n",
        "        1 - \\epsilon + \\epsilon/N    & \\quad \\text{if } a_{t} = \\text{argmax}_{a} \\; q_{t} (a) \\\\\n",
        "        \\epsilon/N        & \\quad \\text{else} \n",
        "        \\end{cases} \n",
        "\\end{align}\n",
        "\n",
        "which is to say that with probability 1 - $\\epsilon$ for $\\epsilon \\in [0,1]$ we select the greedy choice, and otherwise we select an action at random (including the greedy option).\n",
        "\n",
        "Despite its relative simplicity, the epsilon-greedy policy is quite effective, which leads to its general popularity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SjylrnLTgRo"
      },
      "source": [
        "#### Exercise 1: Implement Epsilon-Greedy\n",
        "\n",
        "In this exercise you will implement the epsilon-greedy algorithm for deciding which action to take from a set of possible actions given their value function and a probability $\\epsilon$ of simply chosing one at random. \n",
        "\n",
        "TIP: You may find [`np.random.random`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html), [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) useful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaF4ohh7TgRp"
      },
      "source": [
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  #####################################################################\n",
        "  ## TODO for students: implement the epsilon greedy decision algorithm\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: implement the epsilon greedy decision algorithm\")\n",
        "  #####################################################################\n",
        "  # write a boolean expression that determines if we should take the best action\n",
        "  be_greedy = ...\n",
        "  if be_greedy:\n",
        "    # write an expression for selecting the best action from the action values\n",
        "    action = ...\n",
        "  else:\n",
        "    # write an expression for selecting a random action\n",
        "    action = ...\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "# Uncomment once the epsilon_greedy function is complete\n",
        "# q = [-2, 5, 0, 1]\n",
        "# epsilon = 0.1\n",
        "# plot_choices(q, epsilon, epsilon_greedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "ba6b5634-88e4-4630-c3bd-df66a9964b8c",
        "id": "Xd2M0M8PTgRp"
      },
      "source": [
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=558 height=414 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial2_Solution_6f3c3877_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyaNOJtYTgRq"
      },
      "source": [
        "This is what we should expect, that the action with the largest value (action 1) is selected about (1-$\\epsilon$) of the time, or 90% for $\\epsilon = 0.1$, and the remaining 10% is split evenly amongst the other options. Use the demo below to explore how changing $\\epsilon$ affects the distribution of selected actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1ELJHQwTgRq"
      },
      "source": [
        "#### Interactive Demo: Changing Epsilon\n",
        "\n",
        "Epsilon is our one parameter for balancing exploitation and exploration.  Given a set of values $q = [-2, 5, 0, 1]$, use the widget below to see how changing $\\epsilon$ influences our selection of the max value 5 (action = 1) vs the others. \n",
        "\n",
        "At the extremes of its range (0 and 1), the $\\epsilon$-greedy policy reproduces two other policies. What are they?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Q-gDY7dXTgRq"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact(epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0))\n",
        "def explore_epilson_values(epsilon=0.1):\n",
        "  q = [-2, 5, 0, 1]\n",
        "  plot_choices(q, epsilon, epsilon_greedy, rng_seed=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU_EepRPTgRr"
      },
      "source": [
        "---\n",
        "## Section 3: Learning from Rewards\n",
        "\n",
        "Now that we have a policy for deciding what to do, how do we learn from our actions?\n",
        "\n",
        "One way to do this is just keep a record of every result we ever got and use the averages for each action. If we have a potentially very long running episode, the computational cost of keeping all these values and recomputing the mean over and over again isn't ideal. Instead we can use a streaming mean calculation, which looks like this:\n",
        "\n",
        "\\begin{align}\n",
        "q_{t+1}(a) \\leftarrow q_{t}(a) + \\frac{1}{n_t} (r_{t} - q_{t}(a))\n",
        "\\end{align}\n",
        "\n",
        "where our action-value function $q_t(a)$ is the mean of the rewards seen so far, $n_t$ is the number of actions taken by time $t$, and $r_t$ is the reward just received for taking action $a$.\n",
        "\n",
        "This still requires us to remember how many actions we've taken, so let's generalize this a bit further and replace the action total with a general parameter $\\alpha$, which we will call the learning rate\n",
        "\n",
        "\\begin{align}\n",
        "q_{t+1}(a) \\leftarrow q_{t}(a) + \\alpha (r_{t} - q_{t}(a)).\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTqzS53HTgRs"
      },
      "source": [
        "### Exercise 2: Updating Action Values\n",
        "\n",
        "In this exercise you will implement the action-value update rule above. The function will take in the action-value function represented as an array `q`, the action taken, the reward received, and the learning rate, `alpha`. The function will return the updated value for the selection action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xWI_hzYTgRs"
      },
      "source": [
        "def update_action_value(q, action, reward, alpha):\n",
        "  \"\"\" Compute the updated action value given the learning rate and observed\n",
        "  reward.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received for taking the action\n",
        "    alpha (float): the learning rate\n",
        "\n",
        "  Returns:\n",
        "    float: the updated value for the selected action\n",
        "  \"\"\"\n",
        "  #####################################################\n",
        "  ## TODO for students: compute the action value update\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: compute the action value update\")\n",
        "  #####################################################\n",
        "  # write an expression for the updated action value\n",
        "  value = ...\n",
        "  return value\n",
        "\n",
        "\n",
        "# Uncomment once the update_action_value function is complete\n",
        "# q = [-2, 5, 0, 1]\n",
        "# action = 2\n",
        "# print(f\"Original q({action}) value = {q[action]}\")\n",
        "# q[action] = update_action_value(q, 2, 10, 0.01)\n",
        "# print(f\"Updated q({action}) value = {q[action]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FQdGhSrTgRt"
      },
      "source": [
        "---\n",
        "## Section 4: Solving Multi-Armed Bandits\n",
        "\n",
        "Now that we have both a policy and a learning rule, we can combine these to solve our original multi-armed bandit task. Recall that we have some number of arms that give rewards drawn from Gaussian distributions with unknown mean and unit variance, and our goal is to find the arm with the highest mean.\n",
        "\n",
        "First, let's see how we will simulate this environment by reading through the annotated code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F0PD996TgRu"
      },
      "source": [
        "def multi_armed_bandit(n_arms, epsilon, alpha, n_steps):\n",
        "  \"\"\" A Gaussian multi-armed bandit using an epsilon-greedy policy. For each\n",
        "  action, rewards are randomly sampled from normal distribution, with a mean\n",
        "  associated with that arm and unit variance.\n",
        "\n",
        "  Args:\n",
        "    n_arms (int): number of arms or actions\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "    alpha (float): the learning rate\n",
        "    n_steps (int): number of steps to evaluate\n",
        "\n",
        "  Returns:\n",
        "    dict: a dictionary containing the action values, actions, and rewards from\n",
        "    the evaluation along with the true arm parameters mu and the optimality of\n",
        "    the chosen actions.\n",
        "  \"\"\"\n",
        "  # Gaussian bandit parameters\n",
        "  mu = np.random.normal(size=n_arms)\n",
        "\n",
        "  # evaluation and reporting state\n",
        "  q = np.zeros(n_arms)\n",
        "  qs = np.zeros((n_steps, n_arms))\n",
        "  rewards = np.zeros(n_steps)\n",
        "  actions = np.zeros(n_steps)\n",
        "  optimal = np.zeros(n_steps)\n",
        "\n",
        "  # run the bandit\n",
        "  for t in range(n_steps):\n",
        "    # choose an action\n",
        "    action = epsilon_greedy(q, epsilon)\n",
        "    actions[t] = action\n",
        "\n",
        "    # copmute rewards for all actions\n",
        "    all_rewards = np.random.normal(mu)\n",
        "    # observe the reward for the chosen action\n",
        "    reward = all_rewards[action]\n",
        "    rewards[t] = reward\n",
        "    # was it the best possible choice?\n",
        "    optimal_action = np.argmax(all_rewards)\n",
        "    optimal[t] = action == optimal_action\n",
        "\n",
        "    # update the action value\n",
        "    q[action] = update_action_value(q, action, reward, alpha)\n",
        "    qs[t] = q\n",
        "\n",
        "  results = {\n",
        "      'qs': qs,\n",
        "      'actions': actions,\n",
        "      'rewards': rewards,\n",
        "      'mu': mu,\n",
        "      'optimal': optimal\n",
        "  }\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Bj8k_ATgRu"
      },
      "source": [
        "We can use our multi-armed bandit method to evaluate how our epsilon-greedy policy and learning rule perform at solving the task. First we will set our environment to have 10 arms and our agent parameters to $\\epsilon=0.1$ and $\\alpha=0.01$. In order to get a good sense of the agent's performance, we will run the episode for 1000 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8B9Mq6TgRu"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "n_arms = 10\n",
        "epsilon = 0.1\n",
        "alpha = 0.01\n",
        "n_steps = 1000\n",
        "\n",
        "results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
        "ax1.plot(results['rewards'])\n",
        "ax1.set(title=f'Observed Reward ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
        "        xlabel='step', ylabel='reward')\n",
        "ax2.plot(results['qs'])\n",
        "ax2.set(title=f'Action Values ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
        "        xlabel='step', ylabel='value')\n",
        "ax2.legend(range(n_arms));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0camTljTgRv"
      },
      "source": [
        "Alright, we got some rewards that are kind of all over the place, but the agent seemed to settle in on the first arm as the preferred choice of action relatively quickly. Let's see how well we did at recovering the true means of the Gaussian random variables behind the arms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ec9tNdUTgRw"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(results['mu'], label='latent')\n",
        "ax.plot(results['qs'][-1], label='learned')\n",
        "ax.set(title=f'$\\epsilon$={epsilon}, $\\\\alpha$={alpha}',\n",
        "       xlabel='action', ylabel='value')\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRVIXxtjTgRw"
      },
      "source": [
        "Well, we seem to have found a very good estimate for action 0, but most of the others are not great. In fact, we can see the effect of the local maxima trap at work -- the greedy part of our algorithm locked onto action 0, which is actually the 2nd best choice to action 6. Since these are the means of Gaussian random variables, we can see that the overlap between the two would be quite high, so even if we did explore action 6, we may draw a sample that is still lower than our estimate for action 0.\n",
        "\n",
        "However, this was just one choice of parameters. Perhaps there is a better combination?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E593zbeJTgRw"
      },
      "source": [
        "### Interactive Demo: Changing Epsilon and Alpha\n",
        "\n",
        "Use the widget below to explore how varying the values of $\\epsilon$ (exploitation-exploration tradeoff), $\\alpha$ (learning rate), and even the number of actions $k$, changes the behavior of our agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cAQ6SJ_vTgRx"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact_manual(k=widgets.IntSlider(10, min=2, max=15),\n",
        "                         epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0),\n",
        "                         alpha=widgets.FloatLogSlider(0.01, min=-3, max=0))\n",
        "def explore_bandit_parameters(k=10, epsilon=0.1, alpha=0.001):\n",
        "  results = multi_armed_bandit(k, epsilon, alpha, 1000)\n",
        "  plot_multi_armed_bandit_results(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n228WKtrTgRx"
      },
      "source": [
        "While we can see how changing the epsilon and alpha values impact the agent's behavior, this doesn't give as a great sense of which combination is optimal. Due to the stochastic nature of both our rewards and our policy, a single trial run isn't sufficient to give us this information. Let's run mulitple trials and compare the average performance.\n",
        "\n",
        "First we will look at differet values for $\\epsilon \\in [0.0, 0.1, 0.2]$ to a fixed $\\alpha=0.1$. We will run 200 trials as a nice balance between speed and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQaTt3CNTgRy"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "epsilons = [0.0, 0.1, 0.2]\n",
        "alpha = 0.1\n",
        "n_trials = 200\n",
        "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "for i, epsilon in enumerate(epsilons):\n",
        "  for n in range(n_trials):\n",
        "    results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "    trial_rewards[i, n] = results['rewards']\n",
        "    trial_optimal[i, n] = results['optimal']\n",
        "\n",
        "labels = [f'$\\epsilon$={e}' for e in epsilons]\n",
        "fixed = f'$\\\\alpha$={alpha}'\n",
        "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY6Mb47XTgRy"
      },
      "source": [
        "On the left we have plotted the average reward over time, and we see that while $\\epsilon=0$ (the greedy policy) does well initially, $\\epsilon=0.1$ starts to do slightly better in the long run, while $\\epsilon=0.2$ does the worst. Looking on the right, we see the percentage of times the optimal action (the best possible choice at time $t$) was taken, and here again we see a similar pattern of $\\epsilon=0.1$ starting out a bit slower but eventually having a slight edge in the longer run.\n",
        "\n",
        "We can also do the same for the learning rates. We will evaluate $\\alpha \\in [0.01, 0.1, 1.0]$ to a fixed $\\epsilon=0.1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azg5vWxoTgRy"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "epsilon = 0.1\n",
        "alphas = [0.01, 0.1, 1.0]\n",
        "n_trials = 200\n",
        "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "for i, alpha in enumerate(alphas):\n",
        "  for n in range(n_trials):\n",
        "    results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "    trial_rewards[i, n] = results['rewards']\n",
        "    trial_optimal[i, n] = results['optimal']\n",
        "\n",
        "labels = [f'$\\\\alpha$={a}' for a in alphas]\n",
        "fixed = f'$\\epsilon$={epsilon}'\n",
        "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIsGlLLZTgRz"
      },
      "source": [
        "Again we see a balance between an effective learning rate. $\\alpha=0.01$ is too weak to quickly incorporate good values, while $\\alpha=1$ is too strong likely resulting in high variance in values due to the Gaussian nature of the rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms9SoT6uTgR0"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this tutorial you implemented both the epsilon-greedy descision algorithm and a learning rule for solving a multi-armed bandit scenario. You saw how balancing exploitation and exploration in action selection is crtical in finding optimal solutions. You also saw how choosing an appropriate learning rate determines how well an agent can generalize the information they receive from rewards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oDLiBYVTgR1"
      },
      "source": [
        "---\n",
        "# Part 3\n",
        "## Objectives\n",
        "  \n",
        "In this part you will learn how to act in the more realistic setting of sequential decisions, formalized by Markov Decision Processes (MDPs). In a sequential decision problem, the actions executed in one state not only may lead to immediate rewards (as in a bandit problem), but may also affect the states experienced next (unlike a bandit problem). Each individual action may therefore affect affect all future rewards. Thus, making decisions in this setting requires considering each action in terms of their expected **cumulative** future reward.\n",
        "\n",
        "We will consider here the example of spatial navigation, where actions (movements) in one state (location) affect the states experienced next, and an agent might need to execute a whole sequence of actions before a reward is obtained.\n",
        "\n",
        "By the end of this part, you will learn\n",
        "* what grid worlds are and how they help in evaluating simple reinforcement learning agents\n",
        "* the basics of the Q-learning algorithm for estimating action values\n",
        "* how the concept of exploration and exploitation, reviewed in the bandit case, also applies to the sequential decision setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "f84yJc0VTgR2"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve as conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KAwBXZbRTgR2"
      },
      "source": [
        "#@title Figure settings\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BrCV57MqTgR2"
      },
      "source": [
        "#@title Helper functions\n",
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  if np.random.random() > epsilon:\n",
        "    action = np.argmax(q)\n",
        "  else:\n",
        "    action = np.random.choice(len(q))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "class CliffWorld:\n",
        "  \"\"\"\n",
        "  World: Cliff world.\n",
        "  40 states (4-by-10 grid world).\n",
        "  The mapping from state to the grids are as follows:\n",
        "  30 31 32 ... 39\n",
        "  20 21 22 ... 29\n",
        "  10 11 12 ... 19\n",
        "  0  1  2  ...  9\n",
        "  0 is the starting state (S) and 9 is the goal state (G).\n",
        "  Actions 0, 1, 2, 3 correspond to right, up, left, down.\n",
        "  Moving anywhere from state 9 (goal state) will end the session.\n",
        "  Taking action down at state 11-18 will go back to state 0 and incur a\n",
        "      reward of -100.\n",
        "  Landing in any states other than the goal state will incur a reward of -1.\n",
        "  Going towards the border when already at the border will stay in the same\n",
        "      place.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.name = \"cliff_world\"\n",
        "    self.n_states = 40\n",
        "    self.n_actions = 4\n",
        "    self.dim_x = 10\n",
        "    self.dim_y = 4\n",
        "    self.init_state = 0\n",
        "\n",
        "  def get_outcome(self, state, action):\n",
        "    if state == 9:  # goal state\n",
        "      reward = 0\n",
        "      next_state = None\n",
        "      return next_state, reward\n",
        "    reward = -1  # default reward value\n",
        "    if action == 0:  # move right\n",
        "      next_state = state + 1\n",
        "      if state % 10 == 9:  # right border\n",
        "        next_state = state\n",
        "      elif state == 0:  # start state (next state is cliff)\n",
        "        next_state = None\n",
        "        reward = -100\n",
        "    elif action == 1:  # move up\n",
        "      next_state = state + 10\n",
        "      if state >= 30:  # top border\n",
        "        next_state = state\n",
        "    elif action == 2:  # move left\n",
        "      next_state = state - 1\n",
        "      if state % 10 == 0:  # left border\n",
        "        next_state = state\n",
        "    elif action == 3:  # move down\n",
        "      next_state = state - 10\n",
        "      if state >= 11 and state <= 18:  # next is cliff\n",
        "        next_state = None\n",
        "        reward = -100\n",
        "      elif state <= 9:  # bottom border\n",
        "        next_state = state\n",
        "    else:\n",
        "      print(\"Action must be between 0 and 3.\")\n",
        "      next_state = None\n",
        "      reward = None\n",
        "    return int(next_state) if next_state is not None else None, reward\n",
        "\n",
        "  def get_all_outcomes(self):\n",
        "    outcomes = {}\n",
        "    for state in range(self.n_states):\n",
        "      for action in range(self.n_actions):\n",
        "        next_state, reward = self.get_outcome(state, action)\n",
        "        outcomes[state, action] = [(1, next_state, reward)]\n",
        "    return outcomes\n",
        "\n",
        "\n",
        "def learn_environment(env, learning_rule, params, max_steps, n_episodes):\n",
        "  # Start with a uniform value function\n",
        "  value = np.ones((env.n_states, env.n_actions))\n",
        "\n",
        "  # Run learning\n",
        "  reward_sums = np.zeros(n_episodes)\n",
        "\n",
        "  # Loop over episodes\n",
        "  for episode in range(n_episodes):\n",
        "    state = env.init_state  # initialize state\n",
        "    reward_sum = 0\n",
        "\n",
        "    for t in range(max_steps):\n",
        "      # choose next action\n",
        "      action = epsilon_greedy(value[state], params['epsilon'])\n",
        "\n",
        "      # observe outcome of action on environment\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "\n",
        "      # update value function\n",
        "      value = learning_rule(state, action, reward, next_state, value, params)\n",
        "\n",
        "      # sum rewards obtained\n",
        "      reward_sum += reward\n",
        "\n",
        "      if next_state is None:\n",
        "          break  # episode ends\n",
        "      state = next_state\n",
        "\n",
        "    reward_sums[episode] = reward_sum\n",
        "\n",
        "  return value, reward_sums\n",
        "\n",
        "\n",
        "def plot_state_action_values(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing value of each action at each state.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  for a in range(env.n_actions):\n",
        "    ax.plot(range(env.n_states), value[:, a], marker='o', linestyle='--')\n",
        "  ax.set(xlabel='States', ylabel='Values')\n",
        "  ax.legend(['R','U','L','D'], loc='lower right')\n",
        "\n",
        "\n",
        "def plot_quiver_max_action(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing action of maximum value or maximum probability at\n",
        "    each state (not for n-armed bandit or cheese_world).\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  X = np.tile(np.arange(env.dim_x), [env.dim_y,1]) + 0.5\n",
        "  Y = np.tile(np.arange(env.dim_y)[::-1][:,np.newaxis], [1,env.dim_x]) + 0.5\n",
        "  which_max = np.reshape(value.argmax(axis=1), (env.dim_y,env.dim_x))\n",
        "  which_max = which_max[::-1,:]\n",
        "  U = np.zeros(X.shape)\n",
        "  V = np.zeros(X.shape)\n",
        "  U[which_max == 0] = 1\n",
        "  V[which_max == 1] = 1\n",
        "  U[which_max == 2] = -1\n",
        "  V[which_max == 3] = -1\n",
        "\n",
        "  ax.quiver(X, Y, U, V)\n",
        "  ax.set(\n",
        "      title='Maximum value/probability actions',\n",
        "      xlim=[-0.5, env.dim_x+0.5],\n",
        "      ylim=[-0.5, env.dim_y+0.5],\n",
        "  )\n",
        "  ax.set_xticks(np.linspace(0.5, env.dim_x-0.5, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_xticks(np.arange(env.dim_x+1), minor=True)\n",
        "  ax.set_yticks(np.linspace(0.5, env.dim_y-0.5, num=env.dim_y))\n",
        "  ax.set_yticklabels([\"%d\" % y for y in np.arange(0, env.dim_y*env.dim_x,\n",
        "                                                  env.dim_x)])\n",
        "  ax.set_yticks(np.arange(env.dim_y+1), minor=True)\n",
        "  ax.grid(which='minor',linestyle='-')\n",
        "\n",
        "\n",
        "def plot_heatmap_max_val(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate heatmap showing maximum value at each state\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  if value.ndim == 1:\n",
        "      value_max = np.reshape(value, (env.dim_y,env.dim_x))\n",
        "  else:\n",
        "      value_max = np.reshape(value.max(axis=1), (env.dim_y,env.dim_x))\n",
        "  value_max = value_max[::-1,:]\n",
        "\n",
        "  im = ax.imshow(value_max, aspect='auto', interpolation='none', cmap='afmhot')\n",
        "  ax.set(title='Maximum value per state')\n",
        "  ax.set_xticks(np.linspace(0, env.dim_x-1, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_yticks(np.linspace(0, env.dim_y-1, num=env.dim_y))\n",
        "  if env.name != 'windy_cliff_grid':\n",
        "      ax.set_yticklabels(\n",
        "          [\"%d\" % y for y in np.arange(\n",
        "              0, env.dim_y*env.dim_x, env.dim_x)][::-1])\n",
        "  return im\n",
        "\n",
        "\n",
        "def plot_rewards(n_episodes, rewards, average_range=10, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing total reward accumulated in each episode.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  smoothed_rewards = (conv(rewards, np.ones(average_range), mode='same')\n",
        "                      / average_range)\n",
        "\n",
        "  ax.plot(range(0, n_episodes, average_range),\n",
        "          smoothed_rewards[0:n_episodes:average_range],\n",
        "          marker='o', linestyle='--')\n",
        "  ax.set(xlabel='Episodes', ylabel='Total reward')\n",
        "\n",
        "\n",
        "def plot_performance(env, value, reward_sums):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "  plot_state_action_values(env, value, ax=axes[0,0])\n",
        "  plot_quiver_max_action(env, value, ax=axes[0,1])\n",
        "  plot_rewards(n_episodes, reward_sums, ax=axes[1,0])\n",
        "  im = plot_heatmap_max_val(env, value, ax=axes[1,1])\n",
        "  fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJotdgxYTgR3"
      },
      "source": [
        "---\n",
        "## Section 1: Markov Decision Processes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "cOUMja6nTgR3",
        "outputId": "0f00aa5d-7beb-442a-da6f-4a20e16cbc9e"
      },
      "source": [
        "#@title Video 1: MDPs and Q-learning\n",
        "# Insert the ID of the corresponding youtube video\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"8yvwMrUQJOU\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/8yvwMrUQJOU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/8yvwMrUQJOU?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f826b06fb00>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tmyxxGlTgR4"
      },
      "source": [
        "### Section 1.1: Grid Worlds\n",
        "\n",
        "As pointed out, bandits only have a single state and immediate rewards for our actions. Many problems we are interested in have multiple states and delayed rewards, i.e. we won't know if the choices we made will pay off over time, or which actions we took contributed to the outcomes we observed.\n",
        "\n",
        "In order to explore these ideas, we turn the a common problem setting: the grid world. Grid worlds are simple environments where each state corresponds to a tile on a 2D grid, and the only actions the agent can take are to move up, down, left, or right across the grid tiles. The agent's job is almost always to find a way to a goal tile in the most direct way possible while overcoming some maze or other obstacles, either static or dynamic.\n",
        "\n",
        "For our discussion we will be looking at the classic Cliff World, or Cliff Walker, environment. This is a 4x10 grid with a starting position in the lower-left and the goal position in the lower-right. Every tile between these two is the \"cliff\", and should the agent enter the cliff, they will receive a -100 reward and be sent back to the starting position. Every tile other than the cliff produces a -1 reward when entered. The goal tile ends the episode after taking any action from it.\n",
        "\n",
        "<img alt=\"CliffWorld\" width=\"577\" height=\"308\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial3_CliffWorld.png?raw=true\">\n",
        "\n",
        "Given these conditions, the maximum achievable reward is -11 (1 up, 9 right, 1 down). Using negative rewards is a common technique to encourage the agent to move and seek out the goal state as fast as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYIoyF4kTgR4"
      },
      "source": [
        "---\n",
        "## Section 2: Q-Learning\n",
        "\n",
        "Now that we have our environment, how can we solve it? \n",
        "\n",
        "One of the most famous algorithms for estimating action values (aka Q-values) is the Temporal Differences (TD) **control** algorithm known as *Q-learning* (Watkins, 1989). \n",
        "\n",
        "\\begin{align}\n",
        "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(r_t + \\gamma\\max_\\limits{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big)\n",
        "\\end{align}\n",
        "\n",
        "where $Q(s,a)$ is the value function for action $a$ at state $s$, $\\alpha$ is the learning rate, $r$ is the reward, and $\\gamma$ is the temporal discount rate.\n",
        "\n",
        "The expression $r_t + \\gamma\\max_\\limits{a} Q(s_{t+1},a_{t+1})$ is referred to as the TD target while the full expression \n",
        "\\begin{align}\n",
        "r_t + \\gamma\\max_\\limits{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t),\n",
        "\\end{align}\n",
        "i.e. the difference between the TD target and the current Q-value, is referred to as the TD error, or reward prediction error.\n",
        "\n",
        "Because of the max operator used to select the optimal Q-value in the TD target, Q-learning directly estimates the optimal action value, i.e. the cumulative future reward that would be obtained if the agent behaved optimally, regardless of the policy currently followed by the agent. For this reason, Q-learning is referred to as an **off-policy** method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL4wN9_RTgR4"
      },
      "source": [
        "### Exercise 1: Implement the Q-learning algorithm\n",
        "\n",
        "In this exercise you will implement the Q-learning update rule described above. It takes in as arguments the previous state $s_t$, the action $a_t$ taken, the reward received $r_t$, the current state $s_{t+1}$, the Q-value table, and a dictionary of parameters that contain the learning rate $\\alpha$ and discount factor $\\gamma$. The method returns the updated Q-value table. For the parameter dictionary, $\\alpha$: `params['alpha']` and $\\gamma$: `params['gamma']`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17jPzkhFTgR5"
      },
      "source": [
        "def q_learning(state, action, reward, next_state, value, params):\n",
        "  \"\"\"Q-learning: updates the value function and returns it.\n",
        "\n",
        "  Args:\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing the default parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  # Q-value of current state-action pair\n",
        "  q = value[state, action]\n",
        "\n",
        "  ##########################################################\n",
        "  ## TODO for students: implement the Q-learning update rule\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: implement the Q-learning update rule\")\n",
        "  ##########################################################\n",
        "\n",
        "  # write an expression for finding the maximum Q-value at the current state\n",
        "  if next_state is None:\n",
        "    max_next_q = 0\n",
        "  else:\n",
        "    max_next_q = ...\n",
        "\n",
        "  # write the expression to compute the TD error\n",
        "  td_error = ...\n",
        "  # write the expression that updates the Q-value for the state-action pair\n",
        "  value[state, action] = ...\n",
        "\n",
        "  return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlUy2pZLTgR6"
      },
      "source": [
        "Now that we have our Q-learning algorithm, let's see how it handles learning to solve the Cliff World environment. \n",
        "\n",
        "You will recall from the previous tutorial that a major part of reinforcement learning algorithms are their ability to balance exploitation and exploration. For our Q-learning agent, we again turn to the epsilon-greedy strategy. At each step, the agent will decide with probability $1 - \\epsilon$ to use the best action for the state it is currently in by looking at the value function, otherwise just make a random choice.\n",
        "\n",
        "The process by which our the agent will interact with and learn about the environment is handled for you in the helper function `learn_environment`. This implements the entire learning episode lifecycle of stepping through the state observation, action selection (epsilon-greedy) and execution, reward, and state transition. Feel free to review that code later to see how it all fits together, but for now let's test out our agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eapD_ObTTgR6"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.1,  # epsilon-greedy policy\n",
        "  'alpha': 0.1,  # learning rate\n",
        "  'gamma': 1.0,  # discount factor\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 500\n",
        "max_steps = 1000\n",
        "\n",
        "# environment initialization\n",
        "env = CliffWorld()\n",
        "\n",
        "# solve Cliff World using Q-learning\n",
        "results = learn_environment(env, q_learning, params, max_steps, n_episodes)\n",
        "value_qlearning, reward_sums_qlearning = results\n",
        "\n",
        "# Plot results\n",
        "plot_performance(env, value_qlearning, reward_sums_qlearning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8K6kzI0TgR6"
      },
      "source": [
        "If all went well, we should see four plots that show different aspects on our agent's learning and progress.\n",
        "\n",
        "* The top left is a representation of the Q-table itself, showing the values for different actions in different states. Notably, going right from the starting state or down when above the cliff is clearly very bad.\n",
        "* The top right figure shows the greedy policy based on the Q-table, i.e. what action would the agent take if it only took its best guess in that state.\n",
        "* The bottom right is the same as the top, only instead of showing the action, it's showing a representation of the maximum Q-value at a particular state.\n",
        "* The bottom left is the actual proof of learning, as we see the total reward steadily increasing after each episode until asymptoting at the maximum possible reward of -11.\n",
        "\n",
        "Feel free to try changing the parameters or random seed and see how the agent's behavior changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXYdwdTpTgR6"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this tutorial you implemented a reinforcement learning agent based on Q-learning to solve the Cliff World environment. Q-learning combined the epsilon-greedy approach to exploration-expoitation with a table-based value function to learn the expected future rewards for each state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZZwYYp4TgR_"
      },
      "source": [
        "---\n",
        "# Part 4\n",
        "# Objectives\n",
        "  \n",
        "In this part you will implement one of the simplest model-based Reinforcement Learning algorithms, Dyna-Q. You will understand what a world model is, how it can improve the agent's policy, and the situations in which model-based algorithms are more advantagenous than their model-free counterparts.\n",
        "    \n",
        "* You will implement a model-based RL agent, Dyna-Q, that can solve a simple task;\n",
        "* You will investigate the effect of planning on the agent's behavior;\n",
        "* You will compare the behaviors of a model-based and model-free agent in light of an environmental change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3kNXiT0TgSA"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve as conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GnELwTDOTgSA"
      },
      "source": [
        "#@title Figure settings\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Ji_J7fuzTgSA"
      },
      "source": [
        "#@title Helper functions\n",
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  be_greedy = np.random.random() > epsilon\n",
        "  if be_greedy:\n",
        "    action = np.argmax(q)\n",
        "  else:\n",
        "    action = np.random.choice(len(q))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def q_learning(state, action, reward, next_state, value, params):\n",
        "  \"\"\"Q-learning: updates the value function and returns it.\n",
        "\n",
        "  Args:\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing the default parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  # value of previous state-action pair\n",
        "  prev_value = value[int(state), int(action)]\n",
        "\n",
        "  # maximum Q-value at current state\n",
        "  if next_state is None or np.isnan(next_state):\n",
        "      max_value = 0\n",
        "  else:\n",
        "      max_value = np.max(value[int(next_state)])\n",
        "\n",
        "  # reward prediction error\n",
        "  delta = reward + params['gamma'] * max_value - prev_value\n",
        "\n",
        "  # update value of previous state-action pair\n",
        "  value[int(state), int(action)] = prev_value + params['alpha'] * delta\n",
        "\n",
        "  return value\n",
        "\n",
        "\n",
        "def learn_environment(env, model_updater, planner, params, max_steps,\n",
        "                      n_episodes, shortcut_episode=None):\n",
        "  # Start with a uniform value function\n",
        "  value = np.ones((env.n_states, env.n_actions))\n",
        "\n",
        "  # Run learning\n",
        "  reward_sums = np.zeros(n_episodes)\n",
        "  episode_steps = np.zeros(n_episodes)\n",
        "\n",
        "  # Dyna-Q state\n",
        "  model = np.nan*np.zeros((env.n_states, env.n_actions, 2))\n",
        "\n",
        "  # Loop over episodes\n",
        "  for episode in range(n_episodes):\n",
        "    if shortcut_episode is not None and episode == shortcut_episode:\n",
        "      env.toggle_shortcut()\n",
        "      state = 64\n",
        "      action = 1\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "      model[state, action] = reward, next_state\n",
        "      value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "\n",
        "    state = env.init_state  # initialize state\n",
        "    reward_sum = 0\n",
        "\n",
        "    for t in range(max_steps):\n",
        "      # choose next action\n",
        "      action = epsilon_greedy(value[state], params['epsilon'])\n",
        "\n",
        "      # observe outcome of action on environment\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "\n",
        "      # sum rewards obtained\n",
        "      reward_sum += reward\n",
        "\n",
        "      # update value function\n",
        "      value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "      # update model\n",
        "      model = model_updater(model, state, action, reward, next_state)\n",
        "      # execute planner\n",
        "      value = planner(model, value, params)\n",
        "\n",
        "      if next_state is None:\n",
        "        break  # episode ends\n",
        "      state = next_state\n",
        "\n",
        "    reward_sums[episode] = reward_sum\n",
        "    episode_steps[episode] = t+1\n",
        "\n",
        "  return value, reward_sums, episode_steps\n",
        "\n",
        "\n",
        "class world(object):\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def get_outcome(self):\n",
        "        print(\"Abstract method, not implemented\")\n",
        "        return\n",
        "\n",
        "    def get_all_outcomes(self):\n",
        "        outcomes = {}\n",
        "        for state in range(self.n_states):\n",
        "            for action in range(self.n_actions):\n",
        "                next_state, reward = self.get_outcome(state, action)\n",
        "                outcomes[state, action] = [(1, next_state, reward)]\n",
        "        return outcomes\n",
        "\n",
        "class QuentinsWorld(world):\n",
        "    \"\"\"\n",
        "    World: Quentin's world.\n",
        "    100 states (10-by-10 grid world).\n",
        "    The mapping from state to the grid is as follows:\n",
        "    90 ...       99\n",
        "    ...\n",
        "    40 ...       49\n",
        "    30 ...       39\n",
        "    20 21 22 ... 29\n",
        "    10 11 12 ... 19\n",
        "    0  1  2  ...  9\n",
        "    54 is the start state.\n",
        "    Actions 0, 1, 2, 3 correspond to right, up, left, down.\n",
        "    Moving anywhere from state 99 (goal state) will end the session.\n",
        "    Landing in red states incurs a reward of -1.\n",
        "    Landing in the goal state (99) gets a reward of 1.\n",
        "    Going towards the border when already at the border will stay in the same\n",
        "        place.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.name = \"QuentinsWorld\"\n",
        "        self.n_states = 100\n",
        "        self.n_actions = 4\n",
        "        self.dim_x = 10\n",
        "        self.dim_y = 10\n",
        "        self.init_state = 54\n",
        "        self.shortcut_state = 64\n",
        "\n",
        "    def toggle_shortcut(self):\n",
        "      if self.shortcut_state == 64:\n",
        "        self.shortcut_state = 2\n",
        "      else:\n",
        "        self.shortcut_state = 64\n",
        "\n",
        "    def get_outcome(self, state, action):\n",
        "        if state == 99:  # goal state\n",
        "            reward = 0\n",
        "            next_state = None\n",
        "            return next_state, reward\n",
        "        reward = 0  # default reward value\n",
        "        if action == 0:  # move right\n",
        "            next_state = state + 1\n",
        "            if state == 98:  # next state is goal state\n",
        "                reward = 1\n",
        "            elif state % 10 == 9:  # right border\n",
        "                next_state = state\n",
        "            elif state in [11, 21, 31, 41, 51, 61, 71,\n",
        "                           12, 72,\n",
        "                           73,\n",
        "                           14, 74,\n",
        "                           15, 25, 35, 45, 55, 65, 75]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 1:  # move up\n",
        "            next_state = state + 10\n",
        "            if state == 89:  # next state is goal state\n",
        "                reward = 1\n",
        "            if state >= 90:  # top border\n",
        "                next_state = state\n",
        "            elif state in [2, 12, 22, 32, 42, 52, 62,\n",
        "                           3, 63,\n",
        "                           self.shortcut_state,\n",
        "                           5, 65,\n",
        "                           6, 16, 26, 36, 46, 56, 66]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 2:  # move left\n",
        "            next_state = state - 1\n",
        "            if state % 10 == 0:  # left border\n",
        "                next_state = state\n",
        "            elif state in [17, 27, 37, 47, 57, 67, 77,\n",
        "                           16, 76,\n",
        "                           75,\n",
        "                           14, 74,\n",
        "                           13, 23, 33, 43, 53, 63, 73]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 3:  # move down\n",
        "            next_state = state - 10\n",
        "            if state <= 9:  # bottom border\n",
        "                next_state = state\n",
        "            elif state in [22, 32, 42, 52, 62, 72, 82,\n",
        "                           23, 83,\n",
        "                           84,\n",
        "                           25, 85,\n",
        "                           26, 36, 46, 56, 66, 76, 86]:  # next state is red\n",
        "                reward = -1\n",
        "        else:\n",
        "            print(\"Action must be between 0 and 3.\")\n",
        "            next_state = None\n",
        "            reward = None\n",
        "        return int(next_state) if next_state is not None else None, reward\n",
        "\n",
        "\n",
        "# HELPER FUNCTIONS FOR PLOTTING\n",
        "\n",
        "def plot_state_action_values(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing value of each action at each state.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  for a in range(env.n_actions):\n",
        "    ax.plot(range(env.n_states), value[:, a], marker='o', linestyle='--')\n",
        "  ax.set(xlabel='States', ylabel='Values')\n",
        "  ax.legend(['R','U','L','D'], loc='lower right')\n",
        "\n",
        "\n",
        "def plot_quiver_max_action(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing action of maximum value or maximum probability at\n",
        "    each state (not for n-armed bandit or cheese_world).\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  X = np.tile(np.arange(env.dim_x), [env.dim_y,1]) + 0.5\n",
        "  Y = np.tile(np.arange(env.dim_y)[::-1][:,np.newaxis], [1,env.dim_x]) + 0.5\n",
        "  which_max = np.reshape(value.argmax(axis=1), (env.dim_y,env.dim_x))\n",
        "  which_max = which_max[::-1,:]\n",
        "  U = np.zeros(X.shape)\n",
        "  V = np.zeros(X.shape)\n",
        "  U[which_max == 0] = 1\n",
        "  V[which_max == 1] = 1\n",
        "  U[which_max == 2] = -1\n",
        "  V[which_max == 3] = -1\n",
        "\n",
        "  ax.quiver(X, Y, U, V)\n",
        "  ax.set(\n",
        "      title='Maximum value/probability actions',\n",
        "      xlim=[-0.5, env.dim_x+0.5],\n",
        "      ylim=[-0.5, env.dim_y+0.5],\n",
        "  )\n",
        "  ax.set_xticks(np.linspace(0.5, env.dim_x-0.5, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_xticks(np.arange(env.dim_x+1), minor=True)\n",
        "  ax.set_yticks(np.linspace(0.5, env.dim_y-0.5, num=env.dim_y))\n",
        "  ax.set_yticklabels([\"%d\" % y for y in np.arange(0, env.dim_y*env.dim_x, env.dim_x)])\n",
        "  ax.set_yticks(np.arange(env.dim_y+1), minor=True)\n",
        "  ax.grid(which='minor',linestyle='-')\n",
        "\n",
        "\n",
        "def plot_heatmap_max_val(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate heatmap showing maximum value at each state\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  if value.ndim == 1:\n",
        "      value_max = np.reshape(value, (env.dim_y,env.dim_x))\n",
        "  else:\n",
        "      value_max = np.reshape(value.max(axis=1), (env.dim_y,env.dim_x))\n",
        "  value_max = value_max[::-1,:]\n",
        "\n",
        "  im = ax.imshow(value_max, aspect='auto', interpolation='none', cmap='afmhot')\n",
        "  ax.set(title='Maximum value per state')\n",
        "  ax.set_xticks(np.linspace(0, env.dim_x-1, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_yticks(np.linspace(0, env.dim_y-1, num=env.dim_y))\n",
        "  if env.name != 'windy_cliff_grid':\n",
        "      ax.set_yticklabels(\n",
        "          [\"%d\" % y for y in np.arange(\n",
        "              0, env.dim_y*env.dim_x, env.dim_x)][::-1])\n",
        "  return im\n",
        "\n",
        "\n",
        "def plot_rewards(n_episodes, rewards, average_range=10, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing total reward accumulated in each episode.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  smoothed_rewards = (conv(rewards, np.ones(average_range), mode='same')\n",
        "                      / average_range)\n",
        "\n",
        "  ax.plot(range(0, n_episodes, average_range),\n",
        "          smoothed_rewards[0:n_episodes:average_range],\n",
        "          marker='o', linestyle='--')\n",
        "  ax.set(xlabel='Episodes', ylabel='Total reward')\n",
        "\n",
        "\n",
        "def plot_performance(env, value, reward_sums):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "  plot_state_action_values(env, value, ax=axes[0,0])\n",
        "  plot_quiver_max_action(env, value, ax=axes[0,1])\n",
        "  plot_rewards(n_episodes, reward_sums, ax=axes[1,0])\n",
        "  im = plot_heatmap_max_val(env, value, ax=axes[1,1])\n",
        "  fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Rxw_5xTgSB"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 1: Model-based RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "4rIxHCTmTgSB",
        "outputId": "fdee6f01-9587-4ee3-a685-d29a1d42553d"
      },
      "source": [
        "#@title Video 1: Model-based RL\n",
        "# Insert the ID of the corresponding youtube video\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"zT_legTotF0\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video available at https://youtu.be/zT_legTotF0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://www.youtube.com/embed/zT_legTotF0?fs=1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f826abe54e0>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz_lycWzTgSB"
      },
      "source": [
        "The algorithms introduced in the previous tutorials are all *model-free*, as they do not require a model to use or control behavior. In this section, we will study a different class of algorithms called model-based. As we will see next, in contrast to model-free RL, model-based methods use a model to build a policy.\n",
        "\n",
        "But what is a model? A model (sometimes called a world model or internal model) is a representation of how the world will respond to the agent's actions. You can think of it as a representation of how the world *works*. With such a representation, the agent can simulate new experiences and learn from these simulations. This is advantageous for two reasons. First, acting in the real world can be costly and sometimes even dangerous: remember Cliff World from Tutorial 3? Learning from simulated experience can avoid some of these costs or risks. Second, simulations make fuller use of one's limited experience. To see why, imagine an agent interacting with the real world. The information acquired with each individual action can only be assimilated at the moment of the interaction. In contrast, the experiences simulated from a model can be simulated multiple times -- and whenever desired -- allowing for the information to be more fully assimilated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC592bAkTgSB"
      },
      "source": [
        "### Section 1.1 Quentin's World Environment\n",
        "\n",
        "In this tutorial, our RL agent will act in the Quentin's world, a 10x10 grid world. \n",
        "\n",
        "<img alt=\"QuentinsWorld\" width=\"560\" height=\"560\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial4_QuentinsWorld.png?raw=true\">\n",
        "\n",
        "In this environment, there are 100 states and 4 possible actions: right, up, left, and down. The goal of the agent is to move, via a series of steps, from the start (green) location to the goal (yellow) region, while avoiding the red walls. More specifically:\n",
        "* The agent starts in the green state,\n",
        "* Moving into one of the red states incurs a reward of -1,\n",
        "* Moving into the world borders stays in the same place,\n",
        "* Moving into the goal state (yellow square in the upper right corner) gives you a reward of 1, and\n",
        "* Moving anywhere from the goal state ends the episode.\n",
        "\n",
        "Now that we have our environment and task defined, how can we solve this using a model-based RL agent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6fOtEXUTgSC"
      },
      "source": [
        "---\n",
        "## Section 2: Dyna-Q\n",
        "\n",
        "In this section, we will implement Dyna-Q, one of the simplest model-based reinforcement learning algorithms. A Dyna-Q agent combines acting, learning, and planning. The first two components -- acting and learning -- are just like what we have studied previously. Q-learning, for example, learns by acting in the world, and therefore combines acting and learning. But a Dyna-Q agent also implements planning, or simulating experiences from a model--and learns from them. \n",
        "\n",
        "In theory, one can think of a Dyna-Q agent as implementing acting, learning, and planning simultaneously, at all times. But, in practice, one needs to specify the algorithm as a sequence of steps. The most common way in which the Dyna-Q agent is implemented is by adding a planning routine to a Q-learning agent: after the agent acts in the real world and learns from the observed experience, the agent is allowed a series of $k$ *planning steps*. At each one of those $k$ planning steps, the model generates a simulated experience by randomly sampling from the history of all previously experienced state-action pairs. The agent then learns from this simulated experience, again using the same Q-learning rule that you implemented for learning from real experience. This simulated experience is simply a one-step transition, i.e., a state, an action, and the resulting state and reward. So, in practice, a Dyna-Q agent learns (via Q-learning) from one step of **real** experience during acting, and then from k steps of **simulated** experience during planning.\n",
        "\n",
        "There's one final detail about this algorithm: where does the simulated experiences come from or, in other words, what is the \"model\"? In Dyna-Q, as the agent interacts with the environment, the agent also learns the model. For simplicity, Dyna-Q implements model-learning in an almost trivial way, as simply caching the results of each transition. Thus, after each one-step transition in the environment, the agent saves the results of this transition in a big matrix, and consults that matrix during each of the planning steps. Obviously, this model-learning strategy only makes sense if the world is deterministic (so that each state-action pair always leads to the same state and reward), and this is the setting of the exercise below. However, even this simple setting can already highlight one of Dyna-Q major strengths: the fact that the planning is done at the same time as the agent interacts with the environment, which means that new information gained from the interaction may change the model and thereby interact with planning in potentially interesting ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io9gJ1uVTgSC"
      },
      "source": [
        "Since you already implemented Q-learning in the previous tutorial, we will focus here on the extensions new to Dyna-Q: the model update step and the planning step. For reference, here's the Dyna-Q algorithm that you will help implement:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sw7CCB4TgSC"
      },
      "source": [
        "---\n",
        "**TABULAR DYNA-Q**\n",
        "\n",
        "Initialize $Q(s,a)$ and $Model(s,a)$ for all $s \\in S$ and $a \\in A$.\n",
        "\n",
        "Loop forever:\n",
        "\n",
        "> (a) $S$ &larr; current (nonterminal) state <br>\n",
        "> (b) $A$ &larr; $\\epsilon$-greedy$(S,Q)$ <br>\n",
        "> (c) Take action $A$; observe resultant reward, $R$, and state, $S'$ <br>\n",
        "> (d) $Q(S,A)$ &larr; $Q(S,A) + \\alpha \\left[R + \\gamma \\max_{a} Q(S',a) - Q(S,A)\\right]$ <br>\n",
        "> (e) $Model(S,A)$ &larr; $R,S'$ (assuming deterministic environment) <br>\n",
        "> (f) Loop repeat $k$ times: <br>\n",
        ">> $S$ &larr; random previously observed state <br>\n",
        ">> $A$ &larr; random action previously taken in $S$ <br>\n",
        ">> $R,S'$ &larr; $Model(S,A)$ <br>\n",
        ">> $Q(S,A)$ &larr; $Q(S,A) + \\alpha \\left[R + \\gamma \\max_{a} Q(S',a) - Q(S,A)\\right]$ <br>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNwUNwApTgSC"
      },
      "source": [
        "### Exercise 1: Dyna-Q Model Update\n",
        "\n",
        "In this exercise you will implement the model update portion of the Dyna-Q algorithm. More specifically, after each action that the agent executes in the world, we need to update our model to remember what reward and next state we last experienced for the given state-action pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j2mkV07TgSD"
      },
      "source": [
        "def dyna_q_model_update(model, state, action, reward, next_state):\n",
        "  \"\"\" Dyna-Q model update\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated model\n",
        "  \"\"\"\n",
        "  ###############################################################\n",
        "  ## TODO for students: implement the model update step of Dyna-Q\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement the model update step of Dyna-Q\")\n",
        "  ###############################################################\n",
        "  # Update our model with the observed reward and next state\n",
        "  model[...] = ...\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6PzLAeaTgSD"
      },
      "source": [
        "Now that we have a way to update our model, we can use it in the planning phase of Dyna-Q to simulate past experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSh2UokiTgSD"
      },
      "source": [
        "### Exercise 2: Dyna-Q Planning\n",
        "\n",
        "In this exercise you will implement the other key part of Dyna-Q: planning. We will sample a random state-action pair from those we've experienced, use our model to simulate the experience of taking that action in that state, and update our value function using Q-learning with these simulated state, action, reward, and next state outcomes. Furthermore, we want to run this planning step $k$ times, which can be obtained from `params['k']`.\n",
        "\n",
        "For this exercise, you may use the `q_learning` function to handle the Q-learning value function update. Recall that the method signature is `q_learning(state, action, reward, next_state, value, params)` and it returns the updated `value` table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_qLufzyTgSE"
      },
      "source": [
        "def dyna_q_planning(model, value, params):\n",
        "  \"\"\" Dyna-Q planning\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing learning parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  ############################################################\n",
        "  ## TODO for students: implement the planning step of Dyna-Q\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement the planning step of Dyna-Q\")\n",
        "  #############################################################\n",
        "  # Perform k additional updates at random (planning)\n",
        "  for _ in range(...):\n",
        "    # Find state-action combinations for which we've experienced a reward i.e.\n",
        "    # the reward value is not NaN. The outcome of this expression is an Nx2\n",
        "    # matrix, where each row is a state and action value, respectively.\n",
        "    candidates = np.array(np.where(~np.isnan(model[:,:,0]))).T\n",
        "\n",
        "    # Write an expression for selecting a random row index from our candidates\n",
        "    idx = ...\n",
        "\n",
        "    # Obtain the randomly selected state and action values from the candidates\n",
        "    state, action = ...\n",
        "\n",
        "    # Obtain the expected reward and next state from the model\n",
        "    reward, next_state = ...\n",
        "\n",
        "    # Update the value function using Q-learning\n",
        "    value = ...\n",
        "\n",
        "  return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQNqakzlTgSE"
      },
      "source": [
        "With a way to update our model and a means to use it in planning, it is time to see it in action. The following code sets up the our agent parameters and learning environment, then passes your model update and planning methods to the agent to try and solve Quentin's World. Notice that we set the number of planning steps $k=10$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdQkQNw-TgSE"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "  'k': 10,  # number of Dyna-Q planning steps\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 500\n",
        "max_steps = 1000\n",
        "\n",
        "# environment initialization\n",
        "env = QuentinsWorld()\n",
        "\n",
        "# solve Quentin's World using Dyna-Q\n",
        "results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                            params, max_steps, n_episodes)\n",
        "value, reward_sums, episode_steps = results\n",
        "\n",
        "plot_performance(env, value, reward_sums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNQDUxmZTgSF"
      },
      "source": [
        "Upon completion, we should see that our Dyna-Q agent is able to solve the task quite quickly, achieving a consistent positive reward after only a limited number of episodes (bottom left)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Om4U69STgSF"
      },
      "source": [
        "---\n",
        "## Section 3: How much to plan?\n",
        "\n",
        "Now that you implemented a Dyna-Q agent with $k=10$, we will try to understand the effect of planning on performance. How does changing the value of $k$ impact our agent's ability to learn?\n",
        "\n",
        "The following code is similar to what we just ran, only this time we run several experiments over several different values of $k$ to see how their average performance compares. In particular, we will choose $k \\in \\{0, 1, 10, 100\\}$. Pay special attention to the case where $k = 0$ which corresponds to no planning. This is, in effect, just regular Q-learning.\n",
        "\n",
        "The following code will take a bit of time to complete. To speed things up, try lowering the number of experiments or the number of $k$ values to compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfkDoc8WTgSF"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_experiments = 10\n",
        "n_episodes = 100\n",
        "max_steps = 1000\n",
        "\n",
        "# number of planning steps\n",
        "planning_steps = np.array([0, 1, 10, 100])\n",
        "\n",
        "# environment initialization\n",
        "env = QuentinsWorld()\n",
        "\n",
        "steps_per_episode = np.zeros((len(planning_steps), n_experiments, n_episodes))\n",
        "\n",
        "for i, k in enumerate(planning_steps):\n",
        "  params['k'] = k\n",
        "  for experiment in range(n_experiments):\n",
        "    results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                                params, max_steps, n_episodes)\n",
        "    steps_per_episode[i, experiment] = results[2]\n",
        "\n",
        "# Average across experiments\n",
        "steps_per_episode = np.mean(steps_per_episode, axis=1)\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps_per_episode.T)\n",
        "ax.set(xlabel='Episodes', ylabel='Steps per episode',\n",
        "       xlim=[20, None], ylim=[0, 160])\n",
        "ax.legend(planning_steps, loc='upper right', title=\"Planning steps\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv73jNPjTgSF"
      },
      "source": [
        "After an initial warm-up phase of the first 20 episodes, we should see that the number of planning steps has a noticable impact on our agent's ability to rapidly solve the environment. We should also notice that after a certain value of $k$ our relative utility goes down, so it's important to balance a large enough value of $k$ that helps us learn quickly without wasting too much time in planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN95L3oFTgSG"
      },
      "source": [
        "---\n",
        "## Section 4: When the world changes...\n",
        "\n",
        "In addition to speeding up learning about a new environment, planning can also help the agent to quickly incorporate new information about the environment into its policy. Thus, if the environment changes (e.g. the rules governing the transitions between states, or the rewards associated with each state/action), the agent doesn't need to experience that change *repeatedly* (as would be required in a Q-learning agent) in real experience. Instead, planning allows that change to be incorporated quickly into the agent's policy, without the need to experience the change more than once.\n",
        "\n",
        "In this final section, we will again have our agents attempt to solve Quentin's World. However, after 200 episodes, a shortcut will appear in the environment.  We will test how a model-free agent using Q-learning and a Dyna-Q agent adapt to this change in the environment.\n",
        "\n",
        "<img alt=\"QuentinsWorldShortcut\" width=\"560\" height=\"560\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial4_QuentinsWorldShortcut.png?raw=true\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAzZfb6xTgSG"
      },
      "source": [
        "The following code again looks similar to what we've run previously. Just as above we will have multiple values for $k$, with $k=0$ representing our Q-learning agent and $k=10$ for our Dyna-Q agent with 10 planning steps. The main difference is we now add in an indicator as to when the shortcut appears. In particular, we will run the agents for 400 episodes, with the shortcut appearing in the middle after episode #200.\n",
        "\n",
        "When this shortcut appears we will also let each agent experience this change once i.e. we will evaluate the act of moving upwards when in the state that is below the now-open shortcut. After this single demonstration, the agents will continue on interacting in the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO1LfIleTgSG"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 400\n",
        "max_steps = 1000\n",
        "shortcut_episode = 200  # when we introduce the shortcut\n",
        "\n",
        "# number of planning steps\n",
        "planning_steps = np.array([0, 10]) # Q-learning, Dyna-Q (k=10)\n",
        "\n",
        "# environment initialization\n",
        "steps_per_episode = np.zeros((len(planning_steps), n_episodes))\n",
        "\n",
        "# Solve Quentin's World using Q-learning and Dyna-Q\n",
        "for i, k in enumerate(planning_steps):\n",
        "  env = QuentinsWorld()\n",
        "  params['k'] = k\n",
        "  results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                              params, max_steps, n_episodes,\n",
        "                              shortcut_episode=shortcut_episode)\n",
        "  steps_per_episode[i] = results[2]\n",
        "\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps_per_episode.T)\n",
        "ax.set(xlabel='Episode', ylabel='Steps per Episode',\n",
        "       xlim=[20,None], ylim=[0, 160])\n",
        "ax.axvline(shortcut_episode, linestyle=\"--\", color='gray', label=\"Shortcut appears\")\n",
        "ax.legend(('Q-learning', 'Dyna-Q', 'Shortcut appears'),\n",
        "          loc='upper right');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SabIho8DTgSG"
      },
      "source": [
        "If all went well, we should see the Dyna-Q agent having already achieved near optimal performance before the appearance of the shortcut and then immediately incorporating this new information to further improve. In this case, the Q-learning agent takes much longer to fully incorporate the new shortcut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvp-8-oHTgSH"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this notebook, you have learned about model-based reinforcement learning and implemented one of the simplest architectures of this type, Dyna-Q. Dyna-Q is very much like Q-learning, but instead of learning only from real experience, you also learn from **simulated** experience. This small difference, however, can have huge benefits! Planning *frees* the agent from the limitation of its own environment, and this in turn allows the agent to speed-up learning -- for instance, effectively incorporating environmental changes into one's policy.\n",
        "\n",
        "Not surprisingly, model-based RL is an active area of research in machine learning. Some of the exciting topics in the frontier of the field involve (i) learning and representing a complex world model (i.e., beyond the tabular and deterministic case above), and (ii) what to simulate -- also known as search control -- (i.e., beyond the random selection of experiences implemented above).\n",
        "\n",
        "The framework above has also been used in neuroscience to explain various phenomena such as planning, memory sampling, memory consolidation, and even dreaming!"
      ]
    }
  ]
}